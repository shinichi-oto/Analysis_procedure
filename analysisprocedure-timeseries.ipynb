{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **時系列解析の手順や、その手法**\n## **時系列データ**\n**`1.データの種類`**\n- **単体のデータ**：気温、気圧、株価、為替レート、GDP、消費者物価指数、血圧、脈拍、脳波、地震の波動\n    - **`独立な観測データ`**　:　調査データ、治験データ、多変量解析データ\n    - **`時系列データ`**　　　:　株価データ、自身データ、制御系データ\n        - 時間とともに不規則に変動する現象の記録\n        - 数学的には、時系列は時刻をパラメータとする確率変数の実現値と考える\n    - **`空間(平面)データ`** :　GIS(Geographic Information System)\n    - **`時空間データ`**     :  気象データ、人口流動データ\n    - **`時系列データ`**　　　: **$y_{1},...,y_{N}$**\n        - 時系列を図示、横軸 :時刻 $n$, 縦軸 : $y_{n}$\n        - $N$はデータ数(時系列の長さ)\n    \n**`2.多変量時系列`**\n- 互いに関連する複数の時系列が同時に記録されたもの。\n- 国のGDP、消費者物価指数などの経済指標、特定の観測点で <気温、気圧、風速、雨量> を同時観測した場合\n- 異なる観測点で地震の波形を同時記録した場合\n- 多変量時系列は通常同時に観測された時系列を縦に並べてベクトルとして考える。例えば時刻ｎの時系列をｍ個縦に並べる。\n- 通常同時に観測された時系列を立てに並べて、ベクトルとして考える\n- $y_{n}=\\left. \\begin{bmatrix} 気圧 \\\\気温 \\\\ \\vdots \\\\雨量\\end{bmatrix}\\quad\\right\\} 時刻\\quad{n}\\quadの時系列を\\quad{m}\\quad個縦に並べたもの$\n- 上から${j}$番目の成分を$y_{n}(j)$と表す。${m}$が具体的にわかっている場合には${m}$変量時系列と呼ぶ\n    \n**`3.時系列解析の目的　・　時系列の処理`**\n- **`必要知識`**：変数変換、階差(差分)、季節階差、前年比、前期比、移動平均、欠測値、異常値処理\n    - **`3.1. 可視化(時系列の特徴を捉える)`**\n        - データプロット\n        - **周期性を見る(Cyclic)**\n            - **`時系列の周期(性)`**\n                - **時系列が一定の間隔で同じような変動を繰り返す＜成分＞を持つとき【周期的変動】と呼ぶ**\n                - $S_{n} \\simeq S_{n-p}$ **周期的変動**　　： @\\simeq 近似、漸進的に等しい、ホモトープ\n                - $p$ **周期**\n                - 経済時系列では、1年周期の変動は**季節変動**という\n                - 現実の時系列では、$S_{n} = S_{n-1}$が厳密に成り立つ完全な周期変動は少ない、周期的変動パターンは徐々に変化する\n            - **`時系列の特徴を可視化する方法`**\n                - **スペクトル**　> 時系列を周期関数に分解、その大きさを見る\n                    - 光をプリズムで沢山の色に分解できるように、**時系列はサイン・コサインの和に分解可能**\n                    - **その強さを表示するとどの周期が強いのかわかる**　　@余談： 色の種類をｘ、色の高さをｙ\n                    - 同じように**ランダムな波形も周期関数の和で表現できる**\n                    - スペクトルアナライザや音声のグラフィック・イコライザ\n                    - 波形　＞　スペクトルアナライザ　＞　周期\n                - **相関関数**　> 離れた時点の時系列との相関を見る\n        - `時間的な相関をみる`\n　　- **`3.2. 情報抽出(時系列から情報を取り出す)`**\n        - **`どんな情報に注目するかによって分析手法が違う`**\n            - **`時系列の傾向を見る`**：小さな変動を無視して、トレンドを推定する`<季節調整、移動平均>`\n            - **`短期的な動きを見る`**：トレンド除去後、トレンド周りの動きを抽出`<階差>`\n        - **`解析手法`**\n            - `季節調整手法`\n                - **`トレンド`**\n                    - 時系列の<長期的傾向>をトレンドと呼ぶ\n                    - トレンドの推定方法<季節調整、移動平均>。目的や推定方法によるトレンドの滑らかさは変わる。\n                        - **`移動平均`**\n                            - **音声、画像の信号処理、金融のテクニカル分析で利用される**\n                            - 以下の式で平滑化してゆっくりした傾向変動を取り出す. $y_{n} -> T_{n}$\n                            - $T_{n}=\\frac{1}{2k+1}\\left( y_{n-k} + \\cdots + y_{n} + \\cdots + y_{n+k} \\right)$\n                            - 移動平均は$y_{n}$と前後$k$個 $(合計2k+1個)$のデータの平均\n                            - $K = 2k+1$は項数\n                            - 項数Kを増やす事によって、段々と滑らかになる。項数Kの選び方は重要だが、客観的に決めるのは難しい。\n                            - 5項移動平均や、29項移動平均などのように項数の数によって呼び方が変わる＜n項移動平均＞\n                            - モデル使用で(Prophetなど)綺麗なトレンドが得られる。\n                        - **`階差`**\n                            - 階差は時系列からゆっくりした変動を除去する方法\n                            - **階差とは、$\\Delta y_{n} = y_{n} - y_{n-1}$ という操作、すなわち1時点前の値との差を計算すること**\n                            - **データサイエンスでは＜差分＞が常語**\n                            - 階差によって直線的な動きは除去できる(非定常な時系列は、階差(差分)によって定常化できる事が多い)\n                            - $y_{n} = a + bn$ < 斜め直線 ⇒ $\\Delta y_{n} = y_{n} - y_{n-1} = b$　< 横直線\n                            - **階差(差分)を2回適用すると、2次曲線が除去できる、これを2階階差(差分)**という\n                                - **階差の使用例**\n                                    - 日経255平均株価データの階差を行う。まず、ｙに日経の取引平均額、ｘに日数とする。\n                                    - 階差を使用してトレンドが除去され、短期的変動が浮き彫りになる。\n                                    - これでわかる事は、変動(ボラティリティ)が大きい場所がわかる。\n                        - **`季節階差(差分)`**\n                            - **時系列に季節成分がある場合**、周期$p$の階差(差分)をとる*季節階差*によって**季節変動をある程度除去できる**\n                            - **$\\Delta_{p}y_{n}=y_{n} - y_{n-p}$** <季節階差(季節差分)>\n                            - $p$は季節の長さで、**月次データの場合$p=12$**\n                                - **季節階差(差分)の使用例**\n                                    - 卸売り高データ(ワインの販売数等)の対数をプロットしたデータがあるとする、これはｙが販売総数の対数、ｘが月であるとする。\n                                    - このデータのプロットは販売総数が月次で上がっていくので右上がりのギザギザの線分を確認できるはず。\n                                    - $\\Delta_{p}y_{n}$の季節階差(差分)として計算しなおすと、横ギザ波線にプロットできる。\n                                    - ここからわかる事は、季節階差(差分)によって、仮に$n=40 (40か月)　n=100 (100か月)$ の部分に落ち込みがある場合、\n                                    - **最初の対数プロットではわからなかった販売の落ち込み時期が明確に視覚化され人間にも認知可能になる**\n                                    - *経済時系列の場合*、**階差(差分)と季節階差(差分)の代わりに、<前期化と前年化>を使う事がある**\n                                    - **【Q.】前期化、前年化？**\n                - **`季節成分`**\n                - **`ノイズ`**\n                    - 時系列のうち、取り出したい情報以外の、＜不要な情報＞はノイズと呼ばれる。\n                    - 時系列を２つの成分に分解する時、**何を「取り出したい情報」**, **何を「ノイズ」**とするかは、解析の目的で変わる\n                    - 「見たいとき」とは何を表すかは、何を「予測したいか」という事に言い換えられる。\n                    - **`・時系列の長期的傾向が見たいとき`**　>　【必要情報】: トレンド　【ノイズ】：短期的変動\n                    - **`・直近の動きを見る`** 　　　　　 　　  > 　【必要情報】：短期的変動　【ノイズ】：トレンド\n    - **`3.予測`**\n\n**`4.基礎データの不均衡`**\n- **不均衡データ**\n    - 予測したい事象が非常に少ないデータを「不均衡データ」と呼ぶ。不均衡データの場合は、特別な対応が必要となる。\n        \n    - データ構造に偏りがあり、正または負データの片方が極端に少ないデータ群の事を言う。\n        \n    - (例)：ラベル0：284315(99.83%)　ラベル1: 492(0.17%)　/ のようなデータ例\n        \n- **不均衡データの問題点**\n    - 正例データが非常に多く、負データが極端に少ないデータを用いて予測モデルを構築した場合、予測結果は正となる事が多い。\n        - 予測結果のほとんどが正例となることで、一見精度(Accuracy：正解率)は高く評価されるが、適合率(Precision)・再現率(Recall)は低い結果となる。\n            - Accuray99.0　Recall0　Precision0　のような状態になる\n    - 不均衡データのもう一つの問題として、”余計な”計算コスト\n    \n- **不均衡データを扱う際の注意点**\n    - 不均衡データをそのまま分析、学習に使うのは問題\n        - 不均衡データを扱う際、分析、学習目的に即した対応が必要になる。\n            - 例えば学習の結果、正解率(Accuracy)が高い予測モデルを構築することが目的だった場合を仮定 ：**根本的に分類問題では、正解率は間違い。混同行列が正解**\n                - 負数を予測したい場合、上記の例のような不均衡データを使うと不均衡目的と合致しない予測モデルができあがってしまうことになる。\n                    - **不均衡データを扱う際**には、その主目的、および想定の評価結果に即した対応が必要となる。\n                    \n- **不均衡データの発生原因**\n    - 正または負データの発生確率が極端に低いケースがあげられる。\n        - 非常に稀な病気の発生有無と観測した場合、データの大半は未発症(負)になり、発生者(正)はごく稀\n        - 例えば店舗での取り扱い数の少ない商品分類の購入予測を行う際にも、対象カテゴリーの母数がそもそも少ないことから、その購買履歴は不均衡データとなってしまうケースがある\n\n- **不均衡データに対するアプローチ(代表される５つのアプローチ)**\n    - **アンダーサンプリング**\n        - 多数派のデータを少数派のデータ数に合わせて削除する手法　/ 正と負数を合わせるという事\n        - **注意点**\n            - 多数派のデータを削除している為、重要なデータまでもが欠損し、元の多数派のデータに対しバイアスが生じる原因\n                - 確率予測をする場合、アンダーサンプリングを行ったデータで構築したモデルが出力する予測確率に生じるバイアスを除去し、補助する必要が生じる\n                    - [アンダーサンプリングによるバイアスの補正方法](https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf)\n            - 学習した分類器・予測器の出力の分散が大きくなる(ことがある)という問題がある。\n                - アンダーサンプリング後のデータ量が十分でない場合、的確な予測ができず予測結果にちらばりが生じる\n    - **アンダーバギング**\n        - アンダーバギングという手法は、アンダーサンプリングを実施してk通りの部分集合を作成し、各部分集合ごとに分類器を学習し、バギングでアンサンブルをする手法\n            - バギングは、ブートストラップサンプリング手法を用いて生成したデータを、複数のモデルでそれぞれ学習する方法\n                - **ブートストラップサンプリング**とは、置換を伴うランダムサンプリングの事で、これにより**データの平滑化(偏りをなくす)**を行う\n                - ・$DataSet>\\left. \\begin{bmatrix} 訓練データ \\\\\n                訓練データ\\\\\\vdots\\\\訓練データ\n\\end{bmatrix}\n\\quad\n\\right\\} \\left. \\begin{bmatrix} 予測モデル \\\\\n                予測モデル\\\\\\vdots\\\\予測モデル\n\\end{bmatrix}\n\\quad\n\\right\\} 予測値\n$\n                    - アンダーサンプリングした訓練データに対して、予測を行う。これを複数行い、最終的な予測を一つにバギング(アンサンブル)として纏める\n    - **オーバーサンプリング**\n        - 少数派のデータを多数派のデータ数に合わせて増やしていく手法\n            - 【注意】: オーバーサンプリングの注意点として、無秩序に少数派のデータを増やしても、同じようなデータが増えて過学習に陥る、汎化性能の低い予測モデルを構築してしまう\n            - 【対処】: 対処方法**SMOTE(Synthetic Minority Oversampling TEchnique)**\n                - ランダムサンプリングのデータと、k近傍法のアルゴリズムで求められたデータにより、合成データを作成する方法\n                    - SMOTEはまず少数派のデータからランダムでデータを選択し、そのデータからランダムで選択された近傍点を用いて、両者の合成データを作成する。　：　$K=3(例)$\n                        - ある点を中心として、近傍点を$k=3$選択し、その中からランダムで１つの近傍点を選択し中心点と近傍点の間に新たなサンプルを生成する\n                        \n    - **重み付け**\n        - 少数派のサンプルに重み付けを行い(重要視して)、少数派のカテゴリをより的確に分類できるようにする手法\n            - 重み付けとは、目的関数を誤差に対し(少数派データのラベルごとに)ペナルティを与え、少数派のデータの学習を重視して学習を行う方法\n                - 決定木系のモデルでは、損失関数を元に分類を行うが、そもそも正、負のサンプル少ない場合は、損失に影響を及ぼしにくい為、重み付けを行う必要がある\n        - 重み付けのメリットとして、メモリを節約して不均衡データに対応できるようになることがある。\n            - オーバーサンプリングのように、データ量の増加は発生しない為\n                - 重み付け注意点として、重み付け学習データにのみ行い、検証データ、テストデータに対しては重み付けを行ってはいけない\n                    - 実際のデータの比率に対しての精度を見て評価を行う為。\n    \n    - **異常検知問題として扱う**\n        - あまりにもデータに偏りがありデータが不足しており、尚且つ少数派のデータがクラスターを作れていない場合、分類問題とそて扱うのは困難なケースがある\n            - この場合、分類問題ではなく、異常検知問題としてのアプローチが有効なケースがある\n                - 正常データ群からの距離や密度の違いに着目し、事前に定めた閾値(しきい値)を超えたデータを異常値として判定する： **外れ値検知**\n                    - 正常データ群のうち、異常データ群に該当する箇所\n                - Local Outlier Factor(LOF)\n                    - K近傍法ベースのアルゴリズム\n                - One-class SVM\n                    - サポートベクトルマシンの改良版\n                - Isolation Forest(ISO)\n                    - ランダムフォレストと似たアルゴリズム","metadata":{}},{"cell_type":"markdown","source":"# **`時系列解析では、次のような問題を考える`**\n***\n## **`0.データの種類`**\n- **1.データの種類**\n    - **単体のデータ**：気温、気圧、株価、為替レート、GDP、消費者物価指数、血圧、脈拍、脳波、地震の波動\n        - **独立な観測データ**　:　調査データ、治験データ、多変量解析データ\n        - **時系列データ**　　　:　株価データ、自身データ、制御系データ\n            - 時間とともに不規則に変動する現象の記録\n            - 数学的には、時系列は時刻をパラメータとする確率変数の実現値と考える\n        - **空間(平面)データ** :　GIS(Geographic Information System)\n        - **時空間データ**     :  気象データ、人口流動データ\n        - **時系列データ**　　　: **$y_{1},...,y_{N}$**\n            - 時系列を図示、横軸 :時刻 $n$, 縦軸 : $y_{n}$\n            - $N$はデータ数(時系列の長さ)\n    \n- **2.多変量時系列**\n    - 互いに関連する複数の時系列が同時に記録されたもの。\n    - 国のGDP、消費者物価指数などの経済指標、特定の観測点で <気温、気圧、風速、雨量> を同時観測した場合\n    - 異なる観測点で地震の波形を同時記録した場合\n    - 多変量時系列は通常同時に観測された時系列を縦に並べてベクトルとして考える。例えば時刻ｎの時系列をｍ個縦に並べる。\n    - 通常同時に観測された時系列を立てに並べて、ベクトルとして考える\n        - $y_{n}=\\left. \\begin{bmatrix} 気圧 \\\\気温 \\\\ \\vdots \\\\雨量\\end{bmatrix}\\quad\\right\\} 時刻\\quad{n}\\quadの時系列を\\quad{m}\\quad個縦に並べたもの$\n        - 上から${j}$番目の成分を$y_{n}(j)$と表す。${m}$が具体的にわかっている場合には${m}$変量時系列と呼ぶ\n\n## `0.1 基礎データの不均衡`\n- **不均衡データ**\n    - 予測したい事象が非常に少ないデータを「不均衡データ」と呼ぶ。不均衡データの場合は、特別な対応が必要となる。\n        \n    - データ構造に偏りがあり、正または負データの片方が極端に少ないデータ群の事を言う。\n        \n    - (例)：ラベル0：284315(99.83%)　ラベル1: 492(0.17%)　/ のようなデータ例\n        \n- **不均衡データの問題点**\n    - 正例データが非常に多く、負データが極端に少ないデータを用いて予測モデルを構築した場合、予測結果は正となる事が多い。\n        - 予測結果のほとんどが正例となることで、一見精度(Accuracy：正解率)は高く評価されるが、適合率(Precision)・再現率(Recall)は低い結果となる。\n            - Accuray99.0　Recall0　Precision0　のような状態になる\n    - 不均衡データのもう一つの問題として、”余計な”計算コスト\n    \n- **不均衡データを扱う際の注意点**\n    - 不均衡データをそのまま分析、学習に使うのは問題\n        - 不均衡データを扱う際、分析、学習目的に即した対応が必要になる。\n            - 例えば学習の結果、正解率(Accuracy)が高い予測モデルを構築することが目的だった場合を仮定 ：**根本的に分類問題では、正解率は間違い。混同行列が正解**\n                - 負数を予測したい場合、上記の例のような不均衡データを使うと不均衡目的と合致しない予測モデルができあがってしまうことになる。\n                    - **不均衡データを扱う際**には、その主目的、および想定の評価結果に即した対応が必要となる。\n                    \n- **不均衡データの発生原因**\n    - 正または負データの発生確率が極端に低いケースがあげられる。\n        - 非常に稀な病気の発生有無と観測した場合、データの大半は未発症(負)になり、発生者(正)はごく稀\n        - 例えば店舗での取り扱い数の少ない商品分類の購入予測を行う際にも、対象カテゴリーの母数がそもそも少ないことから、その購買履歴は不均衡データとなってしまうケースがある\n\n- **不均衡データに対するアプローチ(代表される５つのアプローチ)**\n    - **アンダーサンプリング**\n        - 多数派のデータを少数派のデータ数に合わせて削除する手法　/ 正と負数を合わせるという事\n        - **注意点**\n            - 多数派のデータを削除している為、重要なデータまでもが欠損し、元の多数派のデータに対しバイアスが生じる原因\n                - 確率予測をする場合、アンダーサンプリングを行ったデータで構築したモデルが出力する予測確率に生じるバイアスを除去し、補助する必要が生じる\n                    - [アンダーサンプリングによるバイアスの補正方法](https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf)\n            - 学習した分類器・予測器の出力の分散が大きくなる(ことがある)という問題がある。\n                - アンダーサンプリング後のデータ量が十分でない場合、的確な予測ができず予測結果にちらばりが生じる\n    - **アンダーバギング**\n        - アンダーバギングという手法は、アンダーサンプリングを実施してk通りの部分集合を作成し、各部分集合ごとに分類器を学習し、バギングでアンサンブルをする手法\n            - バギングは、ブートストラップサンプリング手法を用いて生成したデータを、複数のモデルでそれぞれ学習する方法\n                - **ブートストラップサンプリング**とは、置換を伴うランダムサンプリングの事で、これにより**データの平滑化(偏りをなくす)**を行う\n                - ・$DataSet>\\left. \\begin{bmatrix} 訓練データ \\\\\n                訓練データ\\\\\\vdots\\\\訓練データ\n\\end{bmatrix}\n\\quad\n\\right\\} \\left. \\begin{bmatrix} 予測モデル \\\\\n                予測モデル\\\\\\vdots\\\\予測モデル\n\\end{bmatrix}\n\\quad\n\\right\\} 予測値\n$\n                    - アンダーサンプリングした訓練データに対して、予測を行う。これを複数行い、最終的な予測を一つにバギング(アンサンブル)として纏める\n    - **オーバーサンプリング**\n        - 少数派のデータを多数派のデータ数に合わせて増やしていく手法\n            - 【注意】: オーバーサンプリングの注意点として、無秩序に少数派のデータを増やしても、同じようなデータが増えて過学習に陥る、汎化性能の低い予測モデルを構築してしまう\n            - 【対処】: 対処方法**SMOTE(Synthetic Minority Oversampling TEchnique)**\n                - ランダムサンプリングのデータと、k近傍法のアルゴリズムで求められたデータにより、合成データを作成する方法\n                    - SMOTEはまず少数派のデータからランダムでデータを選択し、そのデータからランダムで選択された近傍点を用いて、両者の合成データを作成する。　：　$K=3(例)$\n                        - ある点を中心として、近傍点を$k=3$選択し、その中からランダムで１つの近傍点を選択し中心点と近傍点の間に新たなサンプルを生成する\n                        \n    - **重み付け**\n        - 少数派のサンプルに重み付けを行い(重要視して)、少数派のカテゴリをより的確に分類できるようにする手法\n            - 重み付けとは、目的関数を誤差に対し(少数派データのラベルごとに)ペナルティを与え、少数派のデータの学習を重視して学習を行う方法\n                - 決定木系のモデルでは、損失関数を元に分類を行うが、そもそも正、負のサンプル少ない場合は、損失に影響を及ぼしにくい為、重み付けを行う必要がある\n        - 重み付けのメリットとして、メモリを節約して不均衡データに対応できるようになることがある。\n            - オーバーサンプリングのように、データ量の増加は発生しない為\n                - 重み付け注意点として、重み付け学習データにのみ行い、検証データ、テストデータに対しては重み付けを行ってはいけない\n                    - 実際のデータの比率に対しての精度を見て評価を行う為。\n    \n    - **異常検知問題として扱う**\n        - あまりにもデータに偏りがありデータが不足しており、尚且つ少数派のデータがクラスターを作れていない場合、分類問題とそて扱うのは困難なケースがある\n            - この場合、分類問題ではなく、異常検知問題としてのアプローチが有効なケースがある\n                - 正常データ群からの距離や密度の違いに着目し、事前に定めた閾値(しきい値)を超えたデータを異常値として判定する： **外れ値検知**\n                    - 正常データ群のうち、異常データ群に該当する箇所\n                - Local Outlier Factor(LOF)\n                    - K近傍法ベースのアルゴリズム\n                - One-class SVM\n                    - サポートベクトルマシンの改良版\n                - Isolation Forest(ISO)\n                    - ランダムフォレストと似たアルゴリズム\n## **`1. 可視化 (時系列の特徴を捉える)`**\n- 一般的に良く用いられる線形回帰分析というのは、データが正規分布に従うという仮定\n    - **厳密には残差が正規分布に従う**\n        - **データの従う分布が決められているような線形回帰分析などの手法をパラメトリックモデルと呼びます。**\n            - **最近流行りの機械学習手法はノンパラメトリックモデル**\n            - ランダムフォレストやサポートベクター回帰などのノンパラメトリックモデル手法を使って解析することは一つの手\n- **データプロット**\n    - 時系列をそのままPlot\n        - 自己相関関数を何の変換もせずにPlot\n            - 階差を取る\n                - 自己相関関数表示\n                    - 階差のラグを上げる\n                        - 自己相関関数を表示\n    \n- **周期性をみる(Cyclic)**\n    - 時系列が一定の間隔で同変動を繰り返す成分を持つ。**周期的変動** またはその状態を指す\n    - **時系列が一定の間隔で同じような変動を繰り返す＜成分＞を持つとき【周期的変動】と呼ぶ**\n    - $S_{n} \\simeq S_{n-p}$ **周期的変動**　　： @\\simeq 近似、漸進的に等しい、ホモトープ\n    - $p$ **周期**\n    - 経済時系列では、1年周期の変動は**季節変動**という\n    - 現実の時系列では、$S_{n} = S_{n-1}$が厳密に成り立つ完全な周期変動は少ない、周期的変動パターンは徐々に変化する\n        - 経済時系列、１年周期の変動は**季節変動**\n            - **`周期関数`**\n                - サイン・コサインのようなひとつの周期間数で表現できる場合、スペクトルは1点に集中する。\n                - $y_{n}=sin(2\\pi f_{1} n)$ ⇒ $p(f) > f_{1}$ **単一周期関数**\n                - 2つの周期関数の和の場合はスペクトルは２つになる。(つまり周期成分が２つある時)\n                - $y_{n}=sin(2\\pi f_{1} n) + 0.8sin(4 \\pi f_{1} n)$ \n                - barplotの場合、棒の高さは振幅の2乗に比例する\n                - `概念：白色雑音(ホワイトノイズ)`\n                    - **`時間的に相関がない時系列　$R_{k}, k \\neq 0$`**を白色雑音(ホワイトノイズ)と呼ぶ\n                    - 理想的なノイズ\n                    - 時系列モデルは残差が白色雑音になるように構成\n                    - **`時系列が白色雑音の場合`、データ数が$n$のとき自己相関関数の推定誤差分散は$1/n(標準偏差は1/\\sqrt{n})$**\n                    - この性質は時系列が白色雑音かどうかを判断するときに使用可能\n                        - 周期関数を同じ割合で足していくとどうなるか？\n                        - **あらゆる周期の波が同じ割合で含まれているので、スペクトルは一定値になる**\n                        - このようなあらゆる周期の波が均等に含まれる時系列は、**白色雑音**と呼ばれ理想的なノイズと考えられる\n                        - ＜Plot＞：このような線形を可視化すると、一定のｙ高さのプロットが得られる。無限に集めると横棒になる。\n                        - **⽩⾊光はいろいろな波⻑の光が同じ割合で混ざったもの**\n                    - **`【可視化方法(Plot)】`**\n                    - **コレログラム**\n                        - データ変換の度にコレログラムを確認する\n                        - ラグと自己相関を表したグラフ\n                        - ラグとは元データからどれほど時間をずらしているかを表す指標、元データ$x$から時間をずらしたデータ$y$との相関係数を表すグラフ\n                        - 横軸のラグ、縦軸に自己相関\n                        - 塗りつぶされた領域は、データが無相関としたときの95%信頼区間で、この領域の外になる場合この仮定は棄却、相関があると判断できる\n    - **`時系列の特徴を可視化する`**\n        - **`【スペクトル】`**時系列を周期関数に分解し、その大きさを見る↑\n            - 時系列はサイン・コサインの**和**に分解可能。その強さを表示するとどの周期が強いかがわかる。\n            - ランダムな波形も周期関数の和で表現\n        - **`【相関関数】`**離れた時点の時系列との時間的な相関を見る: **<自己相関の認識によって階差その他の調整を行う>**\n        - 相関係数を求めることは、時系列データを使った因果推論や要因探索の第1歩\n            - **`自己相関(ACF：Auto correlation Function)`**\n            - **`自己相関：acorr()`**\n                - 時系列のある時刻とkだけ離れた時刻との相関をみると相関の観点から時系列の特徴を捉えられる、離れた時点の時系列の相関を見れる\n                - 通常の相関と同じで正と負の相関を見て、相関の無さは0に近い値\n                - 時間差$k$はラグと呼ばれる。\n                - 相関図は散布図ではなく線形の波状態で表示される事が多い。これは、片側右上から左に落ちているときは分析手法が割るという事\n                - 【説】時系列のある時刻と$k$だけ離れた時刻との相関を見ると相関の観点から時系列の特徴を捉えることができる。\n                    - 時系列のある時刻と$k$だけ離れた時刻との相関をみると相関の観点から時系列の特徴を捉えることができる\n                    - $y_{n}$と$y_{n-k}$の相関係数を$R(k)$と書く\n                    - **$R(k)=Cor(y_{n}, y_{n-k})$** : **自己相関関数**　：　$y=R(k), x=k$\n                    - $k　時間差$　：　$n (k⇒) n-k $ : $n 右, n-k左$\n                    - $R(k)、k$だけ離れた地点との相関の大きさを表す\n                    - $y_{n}とy_{n-k}$に正の相関があるとき$R(k)>0$\n                    - $y_{n}とy_{n-k}$に負の相関があるとき$R(k)<0$\n                    - $R(k)$を時間差$k$の関数とみなじたものが自己相関関数\n                        - 時間差$k$はラグと呼ぶ\n                        - 周期的変動があるとその周期で$R(k)$も上下する\n                        - 通常、$R(k)$は$k$が正または0の時しか図示しないのは**遇関数**で$R(k)=R(-k)$が常になりたつから。\n                    - ２つの変数間の関係性を表す相関係数がありますが、自己相関（係数）は、1つの変数において、現在の自分と過去の自分の相関係数を計算します。\n                    - 具体的には、下記Pandas関数です。\n                        - $df.shift(1)$ : lag=1 : １つ過去のデータ\n                        - $df.shift(2)$ : lag=2 : ２つ過去のデータ\n                            - このようにデータをズラす事を$lag（ラグ）$、２つずらすなら$lag=2$\n                        - 元のデータ$lag=0$と$lag=1$の相関関係を求めた結果を、$lag1$の自己相関係数と言う\n                            - NaNが発生する行は補完する事も出来るが、削除する事が多い。\n                        - 2020-01-01と2020-02-01の$lag=0, lag=1$の自己相関係数は、1か月前とどれくらいの相関があるかという事\n                    - lag=2,3とlagを増やしていく\n                        - **横軸がlag**、**縦軸が自己相関係数**のこのグラフをコレログラムという。\n                        - lagを1から考えられるlagを変えた時の元データとの相関関係をグラフ化したもの。lag=0の自己相関係数が1なのは元データ同士の相関係数のため。\n                            - **網掛け**という部分は95%信頼区間を表す。\n                                - **網掛け内は相関係数が0**である可能性が高いという事。\n                                - lag=12,24,36と12の倍数で山が出来ている場合、**12か月の周期性**がありそうと解釈できる\n                - **`自己相関関数における階差による相関関数の変数`**\n                    - 自己相関関数の非常にゆっくり減退する年周期は、そのまま分析するのは良くない。\n                    - **自己相関係数が非常に分かりにくい状態の線分を階差を取ってから自己相関関数を見る事によって自己相関係数を確認できる**\n                    - 階差をとると、自己相関関数は著しく変化する。\n                    - 階差によって時系列の性質が変わる。\n            - **`相互相関(偏自己相関：PACF：Partial Auto correlation Function)`**\n            - **`相互相関：xcorr()`**\n                - 1変量の時系列と同様に、多変量時系列の場合には相互相関という\n                - 今日と2日前の関係を見たい歳、間接的に1日前の影響が含まれる為、偏自己相関を用いることで、1日前の影響を除いて今日と2日前だけの関係を調べる事ができるようになる\n                - 自己相関のコレログラムから12か月周期がありそうと考える場合、**これを更に正確に見る場合、偏自己相関を確認する**\n                - 自己相関は、今月と先々月の自己相関を見たい時に、実は先月との関係性が含まれている。\n                    - 偏自己相関は、**先月(対象)との関係性**を取り除き、今月と先々月の直接的な関係を確認する事が出来る。\n                        - lag=0,1は自己相関と同じになる。\n                            - lag=1の場合は間に値がないので、関係性を取り除く必要がないため自己相関と同じになる\n                        - lag=2,3を見ると自己相関のコレログラムと比べ小さい事がわかる\n                            - lag=1や2の関係性が取り除かれている事がわかる\n                        - lag=12を見ると相関がある事がわかる事が、12か月周期だと解釈可能。\n                - **`相互相関`**\n                    - 一変量の時系列と同様に、多変量時系列の場合には $i番目$ の時系列 $y_{n}(i)$ と時間が $k$ だけ離れた $j番目$ の時系列 $y_{n-k}(j)$ の相関係数\n                    - **$R_k(i, j)=Cor(y_{n}(i), y_{n-k}(j))$** を考える。これを**相互相関**と呼ぶ\n                    - $m$ 変量時系列の場合 $m×m$ 行列 $R_{k}$ が定義できる\n                    - $R_{k}=\\begin{bmatrix}R_{k}(1,1) & \\cdots & R_{k}(1,m) \\\\ \\vdots & \\ddots & \\vdots \\\\ R_{k}(m,1) & \\cdots & R_{k}(m,m)\\end{bmatrix}$\n                    - $R_{k}$を時間差$k$の関数とみなしたものが相互相関関数\n                    - $R_{k}$の体格成分$R_{k}(i,i)$は$i番目$の時系列$y_{n}(i)$の自己相関関数\n                    - $R_{k}の(i,j)$成分$R_{k}(i,j)はy_{n}(i)とk時刻前のy_{n-k}(j)$の相関の大きさを表す。ただし、**相互相関は因果関係を示している訳ではない**\n                    - **相関があるからといって因果性があるとは限らない**\n                \n            - ●**傾向変動**の大きさと**周期変動**の振れ幅に相関がある場合、乗法モデルの方がうまくあてはまると言われる。\n            - ●ACF,PACFを正しく解釈する必要がある多段階プロセス。間違ったモデルを使用すると、誤った結果が生じやすくなる。\n    - **`定常性の有無(時系列)`**\n        - 時系列データは不規則な変動をしており、時系列解析ではこのような不規則な変動を確立的なモデルで表現する\n        - データの背後にある確率過程が時間変化に応じて変化しない場合(安定している状態)、定常性がある(定常時系列)(横一直線)という\n        - 明確なトレンドがあったり、周期的に外的な影響が作用して、平均値や分散などが時間的に変化してしまう場合は定常性がない(非定常時系列)という\n        - この場合には、自己回帰型モデルを使用する事が可能\n            - これはつまり、前処理でトレンドを削除してしまえば、自己回帰型モデルを使用可能という事。\n        - **定常性の強弱**\n            - **`弱定常性`**\n                - ①平均が一定、②分散が一定、③自己共分散があるラグｋのみに依存する\n                - ⇒自己共分散、自己相関が時点に依存しないことを意味します。\n            - **`強定常性`**\n                - ①任意のｔとｋにおいて、同時分布が同一である\n                - ⇒同時分布が不変であること、つまりどの時点の確率分布も等しいいことを意味しています。\n        - **`⇒時系列データの構造`**：時系列データは以下の賛成分の合成、傾向変動(トレンド)+周期変動(季節変動)+不規則変動(ノイズ)：⇒**季節性参照**","metadata":{}},{"cell_type":"markdown","source":"***\n## **`2. 情報抽出 (時系列から情報を取り出す)・前処理, 処理は１．で行った結果を元に行う`**\n- **時系列の傾向を見る**\n    - **`どんな情報に注目するかによって分析手法が違う`**\n        - **`時系列の傾向を見る`**：小さな変動を無視して、トレンドを推定する`<季節調整、移動平均>`\n        - **`短期的な動きを見る`**：トレンド除去後、トレンド周りの動きを抽出`<階差>`\n    - **例この時系列データには、上昇傾向のトレンドがある**\n        - **トレンドがある時系列データの場合、トレンドを除去した時系列データで自己相関を計算**\n            - ⇒　トレンドがあった場合、トレンド除去操作を行って再自己相関\n            - トレンドを除去した時系列データの自己相関を求める方法\n        - **手法一覧**\n            - **mlab.detrend_linear**\n                - mlab.detrend_linear/Matplotlib : 線形トレンドを除去\n            - **scipy.signal.detrend**\n                - SciPyトレンド除去関数\n            - **numpy.diff(), pandas.diff()**\n                - NumPy:階差を求める関数:当期と前期の差分を階差\n                - 階差はトレンドが除去された時系列データになる\n            - **statsmodels.tsa.seasonal.STL**\n                - **[STL](https://www.statsmodels.org/devel/generated/statsmodels.tsa.seasonal.STL.html)**\n                - STLという手法で成分分解したが、STLでは加法分解の機能だが、乗法モデルでもよく捉える事が可能\n                - STL関数、LOESS平滑化というノンパラメトリック回帰モデルを利用し分解する。\n                - STL分解は、外れ値に強いなどの特徴がある。\n            - **statsmodels.tsa.seasonal.DecomposeResult**\n                - **[DecomposeResult](https://www.statsmodels.org/devel/generated/statsmodels.tsa.seasonal.DecomposeResult.html#statsmodels.tsa.seasonal.DecomposeResult)**\n            - **scipy.stats.boxcox**\n                - boxcox変換は対数変換などと異なり、多様な分布を正規分布に近づけることが可能。\n                    - **様々なデータに応用可能**\n                        - 一般的に良く用いられる線形回帰分析というのは、データが正規分布に従うという仮定\n                            - **厳密には残差が正規分布に従う**\n                            - **データの従う分布が決められているような線形回帰分析などの手法をパラメトリックモデルと呼びます。**\n                                - **最近流行りの機械学習手法はノンパラメトリックモデル**\n                                    - ランダムフォレストやサポートベクター回帰などのノンパラメトリックモデル手法を使って解析することは一つの手\n                                        - **Boxcox変換前の値に戻す**\n                                            - $from\\quad scipy.special\\quad import\\quad inv\\_boxcox$\n    - **`時系列を２つの成分に分解する`**\n        - 何を「取り出したい情報」、何を「ノイズ」とするかは、解析の目的によって変わる\n    - **`小さな変動を無視してトレンドを推定する`**\n        - **`季節調整`**\n        - **時系列データの構成要素と成分分解**\n            - **3成分は、足し合わせると元のデータになる。**\n            - (月次の)経済データには、しばしば上昇や下降のトレンドと毎月同じような傾向を持つ**季節成分**が含まれている\n            - したがって、元のデータ(原データ)をみただけでは景気の動向や販売量の増減などを的確に判断することが困難なことがある。\n            - **季節調整とは何らかの原因で特定の周期で繰り返す成分を除去して本質的な現象を抽出すること**\n            - 【説】季節調整とは、何らかの原因で特定の周期で繰り返す成分を除去して**本質的な現象を抽出する手法**\n            - 時系列に季節成分がある場合、周期$p$の階差をとる**季節階差(差分)**によって季節変動をある程度除去可能\n            - (例)卸売り高データの対数から季節階差を取る事によって売り上げの落ち込みやその他のパターンが現れる事がある。\n            - 経済時系列の季節階差(差分)\n                - 季節階差の代わりに前期比と前年比を使う\n            - **`季節調整手法`**\n                - **`傾向変動(トレンド)`**\n                    - 移動平均を取る事で傾向変動(トレンド)を推定する\n                    - 移動平均により、時系列データを平滑化(変動小)し、なめらかな傾向変動(トレンド)を取り出せる\n                    - 移動平均を用いてグラフを作成し、長期的な傾向を表す滑らかな曲線が得られ、時系列データの変化のブレが細かく、解析への悪影響が懸念される際に使用される\n                    - 時間とともに単調に増加/現象する変動。(長期的、統計的な変動であり、線形関数または非線形関数の形で表現)\n                        - 時系列の<長期的傾向>をトレンドと呼ぶ\n                    - トレンドの推定方法<季節調整、移動平均>。目的や推定方法によるトレンドの滑らかさは変わる。\n                        - **移動平均**\n                            - **音声、画像の信号処理、金融のテクニカル分析で利用される**\n                            - 以下の式で平滑化してゆっくりした傾向変動を取り出す. $y_{n} -> T_{n}$\n                            - $T_{n}=\\frac{1}{2k+1}\\left( y_{n-k} + \\cdots + y_{n} + \\cdots + y_{n+k} \\right)$\n                            - 移動平均は$y_{n}$と前後$k$個 $(合計2k+1個)$のデータの平均\n                            - $K = 2k+1$は項数\n                            - 項数Kを増やす事によって、段々と滑らかになる。項数Kの選び方は重要だが、客観的に決めるのは難しい。\n                            - 5項移動平均や、29項移動平均などのように項数の数によって呼び方が変わる＜n項移動平均＞\n                            - モデル使用で(Prophetなど)綺麗なトレンドが得られる。\n                        - **階差**\n                            - 階差は時系列からゆっくりした変動を除去する方法\n                            - **階差とは、$\\Delta y_{n} = y_{n} - y_{n-1}$ という操作、すなわち1時点前の値との差を計算すること**\n                            - **データサイエンスでは＜差分＞が常語**\n                            - 階差によって直線的な動きは除去できる(非定常な時系列は、階差(差分)によって定常化できる事が多い)\n                            - $y_{n} = a + bn$ < 斜め直線 ⇒ $\\Delta y_{n} = y_{n} - y_{n-1} = b$　< 横直線\n                            - **階差(差分)を2回適用すると、2次曲線が除去できる、これを2階階差(差分)**という\n                                - **階差の使用例**\n                                    - 日経255平均株価データの階差を行う。まず、ｙに日経の取引平均額、ｘに日数とする。\n                                    - 階差を使用してトレンドが除去され、短期的変動が浮き彫りになる。\n                                    - これでわかる事は、変動(ボラティリティ)が大きい場所がわかる。\n                        - **季節階差(差分)**\n                            - **時系列に季節成分がある場合**、周期$p$の階差(差分)をとる*季節階差*によって**季節変動をある程度除去できる**\n                            - **$\\Delta_{p}y_{n}=y_{n} - y_{n-p}$** <季節階差(季節差分)>\n                            - $p$は季節の長さで、**月次データの場合$p=12$**\n                                - **季節階差(差分)の使用例**\n                                    - 卸売り高データ(ワインの販売数等)の対数をプロットしたデータがあるとする、これはｙが販売総数の対数、ｘが月であるとする。\n                                    - このデータのプロットは販売総数が月次で上がっていくので右上がりのギザギザの線分を確認できるはず。\n                                    - $\\Delta_{p}y_{n}$の季節階差(差分)として計算しなおすと、横ギザ波線にプロットできる。\n                                    - ここからわかる事は、季節階差(差分)によって、仮に$n=40 (40か月)　n=100 (100か月)$ の部分に落ち込みがある場合、\n                                    - **最初の対数プロットではわからなかった販売の落ち込み時期が明確に視覚化され人間にも認知可能になる**\n                                    - *経済時系列の場合*、**階差(差分)と季節階差(差分)の代わりに、<前期化と前年化>を使う事がある**\n                                    - **【Q.】前期化、前年化？**\n                - **`季節変動(シーズナル)`**\n                    - 季節変動を推定する方法として季節階差を取る\n                    - 季節階差とは、トレンド除去後の時系列データから季節変動(季節成分)を求める事\n                    - 【説】階差参照\n                    - 季節成分を分離することにより、トレンド推定よりトレンドの微妙な変化を捉えることができるようになる\n                    - 単位時間で生じる周期(波の動き)変動のこと\n                    - 季節性を見る、X軸で変化を見る。これは仮説立てに役立つ。\n                - **`不規則変動(ノイズ)`**\n                    - 乗法モデル、加法モデルであるかを誤差成分、つまりノイズで感知する事が可能。\n                    - 傾向変動と季節変動を除去することで、不規則変動を推定できる。\n                    - 上記二つでは説明できない不規則かつ短期間に起こる変動のこと\n                    - 時系列のうち取り出したい情報以外の不要な情報はノイズと呼ばれる\n                        - 誤差(ノイズ)は、ホワイトノイズ(意味のない雑音)と外因性(外部の要因)から成る。\n                            - 元の時系列データから、３つの成分に分解してくれるライブラリを使用する\n                                - 時系列のうち、取り出したい情報以外の、＜不要な情報＞はノイズと呼ばれる。\n                                - 時系列を２つの成分に分解する時、**何を「取り出したい情報」**, **何を「ノイズ」**とするかは、解析の目的で変わる\n                                - 「見たいとき」とは何を表すかは、何を「予測したいか」という事に言い換えられる。\n                                - **`・時系列の長期的傾向が見たいとき`**　>　【必要情報】: トレンド　【ノイズ】：短期的変動\n                                - **`・直近の動きを見る`** 　　　　　 　　  > 　【必要情報】：短期的変動　【ノイズ】：トレンド\n            - **`季節調整の手法（詳細）`**\n                - 季節調整では、観測データ$y_{n}$をトレンド、季節成分、不規則成分(ノイズ)の３成分に分解\n                - $y_{n}=t_{n}+S_{n}+W_{n}$\n                - $t_{n} : トレンド成分$\n                - $S_{n} : 季節成分$\n                - $W_{n} : 不規則成分(ノイズ)$\n                    - 季節成分を分離することにより、単なるトレンド推定よりトレンドの微妙な変化を捉えることができるようになる\n                    - **この方法は月次の経済時系列だけでなく、他の周期を持つデータなどへも適用可能**\n                    - **気象データ(24時間)、環境データ(24時間。1週間)**\n                    - **営業データ(1週間)、4半期経済データ(4期)**\n            \n    - ***`短期的な動きを見たい`***\n        - **`短期的変動が必要な情報、トレンドはノイズ`**\n        - トレンドを除去して、トレンド周りの動きを抽出する\n            - **階差**\n                - 階差は時系列からゆっくりした変動を除去する簡単な方法\n                - 階差とは1時点前の差を計算する(1階階差)、階差を2回適用すると2次曲線が除去できる(2階階差)\n                - (差分)\n                - 階差によって直線的な動きは除去(非定常な時系列は階差によって定常化できる)\n                - 階差によってトレンドが除去され，短期的変動が浮き彫りになる\n                - 変動（ボラティリティ）が⼤きなところがわかる\n            - **トレンド**\n                - 時系列の長期的傾向の事をトレンドと言う\n                - トレンドは経済動向分析などに使われる【Q.】どのような経済動向分析？\n                - トレンドの推定方法\n            - **移動平均**\n                - 平滑化し傾向変動　：　項数　： n項移動平均\n                - 項数Kを増やすと段々滑らかになる。\n                - 音声や画像の信号処理、金融のテクニカル分析で利用\n            - **季節調整**\n                 - 季節調整とは何らかの原因で特定の周期で繰り返す成分を除去して本質的な現象を抽出すること\n                     - 季節調整では観測データ$y_{n}$をトレンド、季節成分、不規則成分の３成分に分解\n                     - 【説】：季節成分を分離する事により、単なるトレンド推定よりトレンドの微妙な変化を捉えられる\n                     - 【使用例】：気象データ、環境データ、営業データ、4半期経済データ\n                         - 卸売り高データ：トレンド、季節成分、ノイズ：データをトレンドと季節成分とノイズに分解すると時系列の傾向が見える       \n            - **`トレンド転換点(Prophet)`**\n                - リアルタイムシリーズのトレンドは突然変化する可能性がある、これはデプロイ前にデータの変動があるのと同じ\n                \n    - ***`時系列の長期的傾向を見る`***\n        - **`トレンドが必要な情報、短期的変動はノイズ`**","metadata":{}},{"cell_type":"markdown","source":"- **`前処理1 (時系列データ)`**\n    ***\n    - **時系列特徴量エンジニアリング**\n        - **ドメイン固有特徴量**\n        - **カレンダー特徴量**\n        - **ラグ特徴量**\n        - **エクスパンド特徴量**\n        - **ローリング特徴量**\n    - **対数変換**\n        - 対数変換とは、変動が著しく大きな時系列データに対し、対数値変換によって元の時系列原子データの傾向を保持したまま、値を小さく変換する手法\n            - 変動の分散を一様にし、複雑な時系列でも変数変換で分析が簡単になることがある\n                - $np.log1p()$これは0に対して対数変換を行った際にinfとなるのを避ける\n        - 時系列の将来を予測する。これはどのように行われているかというと**過去の値を使うと簡単に予測できる**\n            - $ 過去の値(m個) = (n-m) - (n-1)$\n            - $n - Prediction(y_{n|n-1}) = Error(\\varepsilon_{n} = \\nu_{n})$ \n            - 時刻$n-1$までの情報を使って求めた $y_{n}$ の予測値を $y_{n|n-1}$ と表し、実際の観測地との差を予測誤差と呼ぶ\n            - **どんな予測式を使うかが重要で、それによって予測の精度が変わる**\n        - 複雑な時系列でも変数変換で分析が簡単になることがある\n            - 現系列のトレンドに比例した値のばらつきが解消される。\n            - 【例】金額、個数、雨量など正値をとるデータの場合、ビットコインの価格などの極端な動きの場合に対数変換を使用する\n                - 分布が対象でない\n                - 平均がよい代表値でない\n                - 値が大きくなると変動も増える\n                    - 対数変換$y=\\log{x}$が有効\n                    - 分布が対数変換によって正規分布に変換され、その分析結果は逆変換によってもとの変数に戻せる\n            - 時系列の対数変換\n                - 経済時系列では、値が増加すると変動幅も大きくなるものが多い\n                    - 対数変換する\n                        - 変動幅がほぼ一定になる\n                        - トレンドが直線的になる\n                            - 分析が楽になる\n            - **上記の場合は、対数変換　$y=\\log x$**\n                - $y=\\log x \\Leftrightarrow x=e^{y}$ : **変換$\\Leftrightarrow$逆変換**\n                - ポワソン分布が対数変換によって**正規分布**になったり、変換した分布を**元の変数、元の分布に戻せる**\n            - ●**`データを対数に変換する理由`**\n                - **株価、破壊確率、雨量などのデータは、その対数をとると、正規分布に(近く)なる**\n                - 正規分布は何かと便利な性質をもつので、このようなデータは**対数に変換してから扱うと扱いやすい**\n                - 回帰分析における最尤推定\n                - 尤度関数を最大化する際に、その自然対数を考えれば**最大化が容易になる事が多い**\n                - 対数関数は単調増加なので、**対数をとってから最大化**しても**元の関数を最大化したときと＜同じ解＞が得られる**\n            - ●**`片側対数グラフ`**\n                - 時間ｔにおける対象ｙは$y=ae^{-bt} (a,b > 0)$\n                - $\\log y = \\log a - bt$、横軸をｔ、縦軸を$log y$ として対象のデータをプロットする事によって直線になる\n            - ●**`両対数グラフ`**\n                - 計算時間 $t$ が問題問題のサイズ $n$ に対して$t \\simeq n^{k}$ のとき、$\\log t \\simeq k \\log n$ なので\n                - 横軸を$\\log t$, 縦軸$\\log n$ として計算時間をプロットすれば、**傾きから $k$ が確認できる**\n                \n    - **差分変換**\n        - 差分変換とはある時点$t$のときの値とその直前の時点$t-1$のときの値の差分\n            - 1時点離れたデータとの差をとる手法。その結果を差分系列、または階差系列\n            - 差分を取ることで、上昇傾向が完全に取り除かれ、平均が0の正常過程の状態に近づけることができるようになる\n            - y軸のスケールが変換前後で変わるのもポイント\n            - 【例】今日の株価と昨日の株価の差を考えたい時などに使用\n    - **対数差分変換**\n        - 対数変換と差分変換の両方の変換を施す手法\n        - 時系列データが変化率や成長率などである場合、対数差分$\\Delta \\log{(y_{t})} = \\log{(y_{t}/y_{t-1})}$を計算して解析する\n                            \n    - **`季節性の確認 ⇒ 定常性確認`**\n        - **定常性と非定常性の違いと可視化による認識の違い**\n            - **定常性とは**　:　時間経過による影響を(強く)受けない状態の事\n                - **定常過程とは** : その確率的な変動の性質が、各時点に依存せず一定であるような確率過程の事\n                    - 各期の平均が普遍であるとみなせる、上昇(下降)の傾向がないデータの事：つまり一定の直進し上限する線分\n                        - 携帯アラームの周波数など、平均0の周辺で一定の分散をもってランダムに振動し続けるデータは、(弱)定常性があると言える\n                        - 反対に、明らかな上昇傾向が存在し続けている株価の動きなどは、定常性があるとは言えない。これは、**非定常過程**という\n            - **非定常過程** 　:　グラフの右側へ進むにつれて変動(上昇傾向)がある\n                - 時系列データについての説明のため、先に基本の4成分(傾向変動等)に触れる\n                    - (成分分解をする前に)そもそも定常性があるのかを確認する必要があるため、まずは定常性の条件である「平均」「分散」を確認する。\n        - 定常な時系列データは時間によって平均や分散、共分散などが変化しない時系列。\n        - トレンド成分や季節成分のある時系列データは、**非定常**\n        - 時間とともにデータの値が大きくなる、時間に依存するので非定常な時系列データ\n        - データの背後にある確率過程が時間変化に応じて変化しない場合(安定)、定常性があるという。\n            - ⇒　古典的な時系列解析手法は、どうすれば非定常な時系列データが定常状態になるかを検討するステップを挟む。\n        - ⇒　そうする事で、トレンド成分を考慮したモデルを作るべきなのか、季節成分を考慮すべきかが分る\n        - 定常性には**強弱**があり、前提とする家庭が変化する。\n            - 明確なトレンドがあったり、周期的に外的な影響が作用して、平均値や分散などが時間的に変化してしまう場合は、定常性がない、非定常性という\n                - **`【定常性の強弱】`**\n                    - **`弱定常性`**\n                        - ①平均が一定、②分散が一定、③自己共分散があるラグ$k$のみに依存する\n                        - ⇒ 自己共分散、自己相関が時点に依存しないことを意味する\n                    - **`強定常性`**\n                        - ①任意の$t$と$k$において、同時分布が同一である\n                        - ⇒ 同時分布が不変であること、つまりどの時点の確率分布も等しいことを意味する\n        - **時系列データが定常仮定を単位根仮定かを確認する手法**に、**ADF(Augmented Dickey-Fuller)検定**がある\n            - [ADF : 単位根検定](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html)\n            - $from\\quad statsmodels.tsa.stattools\\quad import\\quad adfuller$\n        - 多くの通常モデルが正規分布を前提とするように、時系列モデルでは**定常性**を前提としている。\n        - 上記の理由から、**最初に単位根を確認する**\n            - 帰無仮説：単位根仮定　、対立仮説：定常性　とし、P値(有意水準)が0.05以下なら帰無仮説が棄却、定常性とみなる。\n                - 【例】p値が81.5％であった場合、非定常性である。つまりp値は0.05以下でなければならない。\n                    - この場合は、データを前処理変換を行わなければならない。\n            - **時系列が定常かどうかの確認**\n                - **定常の判断**\n                - ⇒　**ADF検定　:　p値　0.05**　\n                    - 帰無仮説 : 時系列は非定常(**単位根をもつ**)\n                    - 対立仮説 : 時系列は定常(**単位根をもたない**)\n                        - 単位根をもつ時系列データを単位根過程\n                            - **単位根過程とは**\n                                - 原系列は非定常だが、**差分をとると定常になるデータ。**\n                                    - **差分**をとるとは、例えば1期前のデータとの差分を計算し、新たな時系列データを作る\n            - ⇒　**単位根検定とは**\n                - DF検定という物があるが、これを拡張したものが**ADF検定**\n                    - ⇒　**ADF検定を実施する**\n                        - **原系列に対するADF検定**を実施する ⇒\n                            - **原系列に対する**ADF検定\n                            - 原系列を**対数変換し得た対数系列**に対するADF検定\n                                - 原系列に対するADF検定は、原系列そのものに対するADF検定\n                                    - **ここで定常であるという検定結果が出る事は無い**\n                            - **対数系列の階差系列(次数1)に対する**ADF検定\n                                - 1期前との差分を計算し新たな時系列データを作り、その新たな時系列に対するADF検定\n                                - 線形トレンドは、階差系列(次数1)の作成で除去\n                            - 対数系列の階差系列(次数1)の**季節階差系列(次数1)に対する**ADF検定\n                                - 12期前(1年前)との差分を計算し新たな時系列データを作り、その時系列に対するADF検定\n        - **変換後の定常性確認の為のADF検定**\n            - 変換後のデータでADF検定を実施した結果を見る\n                - 有意水準に近づいたがまだ0.5に届いていない場合、次の変換として定常性となりえなかった要因を更に取り除く。\n                    - 定常性が何故ないのかを考察する必要がある。\n                        - 特徴量を追加、生産するなどの対応もいる(特徴量エンジニアリング)\n                            - **時系列データにおける特徴量エンジニアリング：月・曜日・週末・祝日・休日**\n                        - 定常、非定常に左右されない分析手法を採用する。\n        - **時系列ADFの流れ**\n        - 時系列データに対して、ADF検定を行って、その結果を元に仮説が棄却がどのようになるかによる\n            - **データ変換**を行った後に、再度**ADF検定**で確認する。\n        - p値（p-value）が0.01より小さいとき1%有意、\n        - 0.05より小さいとき5%有意、\n        - 0.1より小さいとき10%有意\n            - 有意の場合、帰無仮説（時系列は非定常である）を棄却し対立仮説（時系列は定常である）を採択\n            - どの基準（1%有意、5%有意、10%有意）を使うかという問題があります。\n                - 1%有意が最も厳しく、10%有意が最もゆるいです。伝統的に、5%有意を使うケースが多いです。\n                    - 仮に0.8であった場合、帰無仮説（時系列は非定常である）は棄却されません。\n                        - ⇒ 対数変換を行いADF検定\n                            - $np.log(data)$で原系列を対数変換し新しい時系列データ(対数系列)を得る\n                                - ⇒ 再ADF検定\n                                    - 階差系列(次数1)を作ることで線形トレンドを除去。1期前のデータとの差分を計算し求めた時系列データ\n                                    - $data.diff(1).dropna()$\n                                        - ⇒ 再ADF検定\n                                            - 階差系列(次数1)に対し、さらに季節階差(次数1)を取った時系列データ\n                                            - $data.diff(12).dropna()$\n                                                - ⇒ 再ADF検定\n                                                    - 0.05以下で**帰無仮説（時系列は非定常である）は棄却され対立仮説（時系列は定常である）が採択**\n                                                    - **これは定常性であると言える**\n        - 元は0.997という数値だったが0.0になっているので、定常性であるといえるように変換されている。\n        - 有意水準0.05(5%)は届いているので、仮説は棄却出来る。\n            - 0.05%に届いていなければ、その状態では解析を行えない。\n            - この場合は、なぜ定常性がないのかを考察する必要がある。\n            - 特徴量が非常にシンプルなデータセットを用いているが、本来はそれ以上の特徴量を追加・生成するなどの対応も必要\n            - **定常・非定常**に左右されない分析手法を採用する\n        - **時系列因果性検定**\n            - **グレンジャー因果性検定/インパルス応答**\n                - 使用前提**ADF/AEG**\n                - **グレンジャー因果性検定の使用方法**\n                    - **前提**\n                        - **VARモデルを推定する前**に、まず個々の原系列に対して単位根検定を実施\n                        - **根本的にただの回帰関係が見たいだけ**なら、気にせず**VARモデルを計算しても良い**\n                        - **単位根過程が混ざっていなければ**、そのまま**VARモデルを計算して因果性分析**を行う\n                        - **単位根過程が混ざっていたら**、**まず共和分関係（ランク）を推定**\n                        - **共和分関係がなければ**、**差分系列に対してVARモデルを推定して因果性分析**\n                        - **共和分関係があったら**、**VECMを推定してからVARモデルに変換して初めて因果性分析**\n                - **VARモデルを使用したグレンジャー因果性及びインパルス応答**\n                    - **VAR(Vector Autoregressive, ベクトル自己回帰)モデル**とは**ARモデル(自己回帰モデル)** の**多変量版**\n                    - **VARモデル** で、時系列の変数Xと変数Yの間の関係性を検討する事が可能\n                        - **グレンジャー因果性**\n                        - **インパルス応答**\n                - **VAR(Vector Autoregressive, ベクトル自己回帰)**\n                    - 通常ARモデルは1変量の時系列データを扱い、VARモデルは多変量の時系列データを扱う。\n                    - ARモデルの説明変数は過去の自分、1期前、2期前、3期前のデータを説明変数として採用する。\n                        - VARモデルも同様に、説明変数は過去の自分、ARモデルと同様に1期前、2期前、3期前\n                        - VARモデルが扱う時系列データは多変量。同期の他の時系列データを説明変数として採用しない理由は、同期の他の時系列データを説明変数として採用しない。\n                        - 同時期を含めたい場合、**SVAR(Structural Vector Autoregressive)**を利用する。\n                            - **VARモデル**　で　**多変量の時系列データをモデル化** することで、**グレンジャー因果性を検討** することが出来る。\n                                - ⇒ グレンジャー因果性\n                - **グレンジャー因果性**\n                    - グレンジャー因果性は、時系列の変数同士の関係性の一つで、因果ではない\n                    - 因果があればグレンジャー因果性を示したからと言って因果があるとは限らない。\n                        - ２つの時系列の変数XとYがあるとし、以下のようなものを**グレンジャー因果性があるという**/この3つは、ほぼ同じことを、表現を変えていっています。\n                            - **Yを予測するとき、過去のXのデータを利用すると予測精度があがる**\n                                - 売上を予測するとき、1週間前の広告の投入量を利用すると予測精度があがる\n                            - **Xが、Yの先行指標になっている**\n                                - 1週間前の広告の投入量が、売上の先行指標になっている\n                            - **過去のX（Xのラグ）が、Yと相関している**\n                                - 1週間前の広告の投入量が、売上と相関している\n                        - グレンジャー因果性があるかどうかは、**統計的仮説検定**を使用する\n                            - **帰無仮説：グレンジャー因果性がない**\n                            - **対立仮説：グレンジャー因果性がある**\n                        - **見せかけの回帰が生じる場合**\n                            - このような場合、グレンジャー因果性の検討ができない。\n                                - 「見せかけの回帰」とは、全く無関係な時系列の変数Xと変数Yで回帰モデルを構築（目的変数：Y、説明変数：X）したとき、\n                                - XとYの間に有意な関係があるとみなされ、予測精度の高い回帰モデルができてしまう現象です。\n                                    - このような現象は、時系列の変数Xと変数Yが**単位根過程である場合に起こる**\n                                        - **単位根過程**は、原系列は非定常だが、差分をとると定常になる時系列データ ⇒ **差分**\n                                            - 単位根過程かどうかを判別するために、統計的仮説検定（単位根検定）などを実施\n                                            - 単位根過程である時系列の変数は、階差処理（差分計算）を実施し階差系列にした後にグレンジャー因果性の検討を実施したほうがいい\n                - **インパルス応答**\n                    - 「ある時系列の変数Xに変動があったとき、他の時系列の変数Yにどう伝わっていくかをモデリングする」ことで、時系列の変数Xと変数Yの関係性を検討\n                    - 時系列の変数Xの値を変動させたときの、他の時系列の変数Yの変動を、**インパルス応答**\n                        - VARモデルからインパルス応答関数を求め、インパルス応答をグラフ化\n                        - グレンジャー因果性の統計的仮説検定を実施することなく、インパルス応答のグラフ化から、時系列の変数Xと変数Yの間の関係性\n                            - **インパルス応答関数**\n                                - **非直交化インパルス応答関数**\n                                - **直交化インパルス応答関数**\n                                    - 「広告の投入量に対する、売上の変動を分析する」","metadata":{}},{"cell_type":"markdown","source":"- **`前処理2(特徴量エンジニアリング)`**\n# **`特徴量エンジニアリング`**\n- **時系列データの特徴量エンジニアリング**\n    - **ドメイン固有特徴量**\n    - **カレンダー特徴量**\n        - 平日、週末、休日、季節、四半期、月始、年始、年末、etc...\n        - 午前、午後、昼、朝、夕方\n    - **ラグ特徴量**\n        - **t期の売上**を予測する場合、前月の**t-1期の売上を特徴量として使用**、前々月の場合**t-2期の売上を特徴量**\n        - **t-1期の売上**や**t-2期の売上をラグ特徴量**\n        - ラグ特徴量は、目的変数Yの値さえあれば作成できる\n        - **t期の気温を予測したい場合**、前日t期の気温を特徴量として使用したり、前々日t-2期の気温を特徴量として使用する。\n            - **shift()**\n                - **ラグ1の特徴量 : t-1期の作成**　:　shift(1)\n                - **ラグ2の特徴量 : t-2期の作成**　： shift(2)\n            - **ラグの選択**　⇒ ACF/PACF項確認\n                - **ACF** \n                - **PACF**\n    - **エクスパンド特徴量**\n        - **過去すべての期間**の集計値を**特徴量**としたもの。\n            - **ローリング特徴量**と違い、集計対象となる過去の期間が異なる。\n            - **expanding()**\n                - エクスパンド特徴量 : **過去全期間**\n                - ローリング特徴量 : **決められた一定期間**\n    - **ローリング特徴量**\n        - **過去の一定の期間**の**集計値を特徴量(説明変数)**としたもの\n        - **１つの期間**だけでなく、過去3か月、過去半年、過去1年間など、複数の期間を定めて作る\n            - **rolling()**\n                - **ローリング特徴量**：過去3期を対象とするrolling(3)\n            - **月単位**の時系列データ\n                - **t期の目的変数Yの特徴量**として、**過去3か月間の目的変数Yの平均値や最大値、最小値、標準偏差などを特徴量(説明変数)**とすること。\n                - **目的変数Y以外の時系列データ**の過去3か月間の平均値や、最大値、最小値、標準偏差\n            - **日単位**の時系列データ\n                - **t期の目的変数Yの特徴量**として、**過去1週間の目的変数Yやほかの時系列データの平均値、最大値、最小値、標準偏差などを特徴量**とする。\n- **カテゴリー変数**    \n    - **(カテゴリー変数)：序数エンコーディング**\n        - **[序章エンコーディング](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)**\n    \n    - **(カテゴリー変数)：ワンホットエンコーディング**\n        - **[ワンホット・エンコーディング](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)**\n\n- **数値変換**\n    - **標準化(Standardization)**\n        - [**StandardScaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n        - 数値変数を平均0、分散1にスケーリングする。\n    - **正規化(Min-Max Normalization)**\n        - [**MinMaxScaker**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n        - 最小値0~最大値1のスケーリングする。\n    - **対数変換(Log transformation)**\n        - 対数変換は、数学の自然対数に変換する。\n            - **通常の自然対数のlog()関数に0を入力すると、-inf(マイナス無限大)が返るので、入力に+1した自然対数を計算する**\n            - **log1p()関数**を使用する。この関数に0を入力すると、0が返る。-infの問題は解決する。\n    - **多項式と交互作用の特徴量(Polynomial and interaction feature)**\n        - [**PolynomialFeautures**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n            - **多項式特徴量**などの特徴量の追加は、非線形データセットの処理方法の１つ。**多くのデータセットは線形分割などできない**\n                - これで線形分割可能なデータセットが得られる場合があるのでSVM、非線形SVMなどでデータセットを使用できるようになる。\n                - 特徴量が１つの単純なデータセットだが、線形分割できない。第二の特徴量を追加して2次元データセットにすれば、完全に線形分割可能になる\n                - これをPipelineで実装する場合には、スケーラーの前に設置すればよい\n            - 多項式と交互作用の特徴量(Polynomial Feauture)とは、例えば$[x, y, z, ……]$のような既存の特徴量から\n                - バイアス項：１\n                - 指定する次数までのべき乗：$x^{1}, x^{2}, x^{3}$\n                - 全ての特徴量ペア間の相互作用：$x*y, x*z$\n            - このような法則で、新たな特徴量を作成する手法。\n                - つまり、相関のあるデータを自作生成する事によってデータ数を増やすという方法。\n            - degree=2 (指定する次数が2)\n            - interaction_only=True (相互作用のみで、2乗以降のべき乗の値を含めない)\n            - include_bias=False (バイアス項を含めない)\n    - **ターゲットエンコーディング**\n        - ターゲットエンコーディングに対しては記載せず。自作関数必須\n\n- **欠損値処理**\n    - **リストワイズ法**\n        - 欠損レコードを除去\n    - **ペアワイズ法**\n        - 相関係数など2変数を用いて計算を行う際に、対象の変数が欠損している場合に計算対象から除外\n    - **平均代入法**\n        - 欠損を持つ変数の平均値を補完\n        - 欠損データに別の値を代入する\n    - **回帰代入法**\n        - 欠損を持つ変数の値を回帰式をもとに補完\n        - 回帰代入では欠損が発生している変数と 欠損の発生に影響している変数とで回帰式を作り、作られた回帰式を使って欠損を補完\n        - 欠損は MAR で発生している必要がある\n    - **`発生メカニズム`**\n        - **`Missing Completely At Random : MCAR`**\n            - 完全にランダムで欠損するケース\n            - アンケート回答漏れ・忘れ\n        - **`Missing At Random: MAR`**\n            - 他の項目と関連してランダムで欠損するケース\n            - 性別女性：体重の入力拒否・未回答\n        - **`Missing Not At Random: MNAR`**\n            - その他の項目、項目自体に関連して欠損するケース\n            - 低・高所得者ほど、未入力拒否・未回答が多いなど\n            - 既婚者のみを対象とする項目への回答など\n    - **`欠損データの分類`**\n        - 欠損データを、**「無視可能」か「無視不可能」** かを分けて解析する\n            - 「無視可能」：欠損データの解析結果と欠損していなかった場合の解析結果、その両者に違いがない状態の事を言う。\n            - 両者に違いがあれば、「無視不可能」となる。\n                - **`無視可能`**な欠損データは、欠損データの発生メカニズムが「MCAR」「MAR」の状態を指す。\n                - **`無視不可能`**な欠損データは、「MNAR」の状態を指す。欠損データの発生原因やその影響を分析した上で、欠損データに対する対処手法を決定する\n                    - **`無視不可能時の処理`**\n                        - **`削除`**\n                            - リストワイズ削除法：欠損データを含む行・列を全て削除する\n                                - 欠損したデータ量が少ない際は、解析結果への影響が少ない場合がある\n                                - 欠損データの発生メカニズムがMCAR(完全ランダム)の場合は、欠損データを無視可能となるが、MARの場合は、無視不可能となる場合がある\n                                - 削除法は、欠損データ量、発生原因を考慮して行う必要がある\n                            - ペアワイズ削除法：欠損データの少ない列を残し、そこから欠損している行のみを削除する\n                        - **`補完する`**\n                            - 欠損データを何らかの値で補完する手法：完全なデータセットに近い解析結果を得る\n                            - **「単一代入量」「多重代入法」**\n                                - **`「単一代入量」`**\n                                    - 単一代入法では一般にデータの分散を過小評価しがちになる。欠損値の分散はゼロになる。この対処方法に**多重代入法**で対処する\n                                    - **`平均値代入法`**\n                                    - **`比例代入法`**\n                                    - **`回帰代入法`**\n                                    - **`確立的回帰代入法`**\n                                    - **`ホットデック法`**\n                                        - 類似するデータを補完する手法\n                                            - 類似した背景を持つ回答者を値を、欠損データに補完する\n                                            - 回答者の属性等を解析結果に反映する\n                                                - **欠損データが多い場合**：類似データの割り出しが難しく、解析結果に偏りが生じることもあり、適さない手法になる\n                                - **`「多重代入法」`**\n                                    - 1つの欠損データに対し、浮く数の値を代入する手法\n                                    - 多重代入法では欠損値を埋めた疑似的完全データセットを複数作る。疑似的完全データセットを用いて別個に推定を行う。\n                                        - 複数の推定結果を統合して最終的な推定結果とするのがこの方法。\n                                            - 多重代入法を用いる事でデータの持つ不確実性や分散を適切に評価できると考えられる。\n                                    - **`メリット`**は欠損データが生じる不確実性を考慮した推計を行うことができる\n                                    - **`デメリット`**変数の種類や分析の目的に応じて適切な手法を選択しないと、結果にバイアスが入る可能性がある\n                                        - **`EMアルゴリズムによる補完`**\n                                        - **`マルコフ連鎖モンテカルロ法`**\n                                        - **`DA(Data Augmentation)アルゴリズム`**\n                                        - **`FCS(Fully Conditional Specification)アルゴリズム`**\n                                        - **`EMB(Expection-Maximization with Boot-strapping)アルゴリズム`**\n                        - **`その他`**\n                            - **`完全情報尤度推定法`**\n                                - 統計モデルなどを仮定することで尤度に基づいた解析を行う手法\n- **`クラスタリング`**\n    - あるデータをなんらかの規則に従ってグループ分けすること。グルーピングされたそれぞれのデータ群をクラスタと呼ぶ\n    - 教師なし学習\n        - ECサイトの顧客分析\n            - マーケティング活動の効率化を目的として、市場における自社顧客をグループ化(セグメンテーション)やその可視化を行う際にクラスター分析として用いる\n        - 機械学習を取り入れたAIシステムの構築\n            - データセットの作成工程では、構築モデルの精度向上を目的として、特徴量の生成に用いられることがある。\n    - **`クラスタリングの種類`**\n        - **`階層的クラスター分析`**\n            - **`階層クラスタリング`**\n                - 最も似ているサンプル同士を１つずつ順番にグルーピングしていく手法。クラスター同士の類似度を測る方法\n                    - **`重心法`**\n                        - ２つのクラスターの互いの重心間の距離をクラスター間の距離として類似度を測る方法\n                    - **`最短(最長)距離法`**\n                        - ２つのクラスターに含まれるデータの中で、最もお互いに近い(遠い)データ同士の距離をクラスター間の距離として類似度を測る方法\n                    - **`群平均法`**\n                        - 各クラスター同士で、全ての組み合わせのサンプル間距離の平均をクラスター間距離として類似度を測る方法：**加重平均**\n                - **`可視化(Plot)`**\n                    - デンドログラム\n        - **`非階層的クラスター分析`**\n            - ビジネスで用いられる\n                - **`非階層クラスタリング`**\n                    - あらかじめ決めたクラスタ数に、データを分割する手法\n                        - **`k-means法`**\n                            - あらかじめいくつのクラスターに分類するかを指定し、クラスター内では分散が小さく、クラスター間では分散が大きくなるようにデータを分割する(クラスターに振り分ける)\n                            - **`クラスタリングの注意点`**\n                                - 事前に分析の目的や仮説を明確にしておかなければ、目的や仮説を明確にしておく事。\n                                    - 目的や仮説がないままに分析を実施しても、データ分類することに終始し、それ以上の考察を得られない可能性がある。\n                                    - データによっては特徴量を「平均0、標準偏差1に標準化」する必要がある。\n                                        - スケールの異なるデータに対して算出した(ユークリッド)距離同士を均等に解釈できない為\n                            - **`【メリット】`** : 階層クラスタリングと比べてデータ間の距離を計算する必要がなくなるため、計算コストが低く、大規模なデータに対しても実行速度が速いという長所がある\n                            - **`【デメリット1】`**：最初の重心の指定はランダム、同じ母集団でも計算する度に分類結果が少し変わる。外れ値があるとデータの拡散、クラスタ同士が離れ正しく分類できない\n                            - **`【デメリット2】`**：複雑なデータや、特定の方向に分散したデータをうまく分類できないケースがある\n                                - **`ステップ１`**\n                                    - 初期値となる重心点を、指定数だけ、ランダムで決定・配置\n                                - **`ステップ２`**\n                                    - 各データから最も近い重心点を算出し、クラスタを構築\n                                        - 重心点までの距離の計算には「ユークリッド距離」「マンハッタン距離」「コサイン距離」を用いる\n                                - **`ステップ３`**\n                                    - ２で構成したクラスタごとに重心を再度算出する\n                                - **`ステップ４`**\n                                    - ３で求めた重心が２で求めた重心と異なる場合は、再度２～３を実施。これを２と３の変化がなくなるまで繰り返し実行\n- **`主成分分析`**\n    - あるデータ群をより少ないデータ群を要約する解析手法\n        - **`次元の呪い`**\n            - データの次元数が大きくなり過ぎると、そのデータで表現できる組み合わせが飛躍的に多くなってしまい、その結果十分な学習結果が得られなくなる事を言う\n            - 結果的に、未知のデータに対する予測精度(汎化性能)を高めづらくなり、また計算負荷が非常に高くなる事が問題点\n        - **`次元削減`**\n            - データ数が非常に多いデータセットでは、次元削減という手法が有効となる\n                - 可能な限りデータ元のデータの情報を保持したまま、低次元のデータに落とし込むことをいう\n                - 主成分分析(PCA)における「次元削減」とは、少数の主要な特徴量の線形結合によって、行列の列空間の本質的な次元を表す事。\n                    - 線形結合とは2本のベクトルを組み合わせる事で、2本のベクトルを起点とした第3のベクトルを描く事。\n                        - 組み合わせる2本のベクトルを基底ベクトルと言い、基底ベクトルを用いて空間上のベクトルを描いていくのが主成分分析の基本となる考え方\n- **`相互作用特徴量`**\n    - ペアワイズ相互作用特徴量\n        - **`メリット`**：2つ異常の特徴量の組み合わせにより、目的変数をうまく表現できる場合、単一の特徴量よりモデルの精度が高まる\n        - **`デメリット`**：元の項目数が「$n個$」の場合、特徴量の生成後の項目数は「$n^{2}$乗個」となる為、学習コスト増大する\n- **`特徴量選択`**\n    - **アプローチ**\n        - **`統計量を用いた特徴量選択`**\n            - **相関の確認(可視化)(ペアプロット)**\n            - **相関の確認(可視化)(ヒートマップ)**\n            - **相関の確認(可視化)(バブルチャート)**\n            - **相関の確認(可視化)(散布図)**\n            - **相関の確認(可視化)(グラフネットワーク)**\n            - **相関の確認(時系列の相関)(コレログラム)**\n            - 変数同士の相関関係の強さを表す「相関係数」という指標を用いた特徴選択の手法\n                - 目的変数に対する相関係数の上位の特徴量を選択する。\n                    - 前提として、変数同士にどのような相関関係があるかを正しく捉えられているかが重要。一見相関係数が低いものの中には相関性がある変数があったり、反対に、疑似相関の変数がある\n                    - **`疑似相関`**:一見相関係数の高い変数同士に、実際には相関がないものの事を言う。\n                        - **`ドメイン知識`**があれば検知できることが実際は多い。\n                        - **`ピアソンの積率相関係数`**\n                            - **`相関係数の有意性の検証`**\n                                - ２つの変数の同時分布が「2変量正規分布」に従う事(２つの変数がそれぞれ正規分布に従うか)が必須。\n                                    - **`正規分布の確認`**\n                                        - ヒストグラムで外形の確認\n                                            - 得られたデータから平均と分散、標準偏差\n                                        - **`QQプロット(qqnorm)`**:正規分布の累積分布関数を用いる方法。\n                                            - 累積分布の25%点と75%点を結び、その直線状にデータが分布するかを確認することで、正規分布と近似するかどうかを判断する\n                                        - **コルモゴロフ・スミルノフ検定**\n                            - **母集団が正規分布を仮定した手法**のため、現実の多くのデータに不適切なケースが多々ある。\n                            - 線形な関係があるかどうかを確認することができる。\n                            - データの外形をプロットで確認してからピアソンの積率相関係数を求める必要がある。\n                                - **メリット**：全てのデータの大小関係を考慮できる、間隔尺度・比率尺度の状態を保持して関係性を考察可能\n                                - **デメリット**：ある変数が外れ値を含む場合、標準偏差、分散式はすべての値を計算に含めるため、その結果が外れ値の影響を強く受ける。外れ値処理か順位相関係数使用\n                        - **スピアマンの順位相関係数**\n                            - **相関係数の有意性の検証**\n                            - これは、一緒に変化を起こすかどうか、を確認することができる。\n                                - **メリット**：ピアソンの相関係数と違い、**募集さんの特定の分布仮定しない**ため、**現実の多くのデータに対して有効な分析手法**。外れ値の影響を受けない。\n                                - **デメリット**：順位同士の差が均一化、大小関係の程度は消失。順位付けを行うため、順序尺度のデータに対して有効な手法である事に注意(間隔・比率尺度ではない)\n                        - **無相関検定(相関係数の有意性検定)**\n                            - 母相関係数の無相関検定\n                                - 標本では相関がある場合に、母集団でも同様に相関があるかどうかを確認する\n                                    - **帰無仮説：母相関係数は0である(無相関)**\n                                    - **対立仮説：母相関係数は0でない(相関あり)**\n                                    - (p値<有意水準0.05)で帰無仮説を棄却、対立仮説を採択する\n                        - **相互情報量**\n                            - 母集団に正規分布を仮定しない、非線形な相関関係も検出できる手法がある。\n        - **反復特徴量選択法**\n            - 反復特徴量選択法とは、説明変数を増やしたり減らしたりを繰り返して予測精度を推し量りながら特徴量を選択する手法\n            - 【前提】：統計量を用いた特徴量選択で相関関係の有無、強さを推定済みである事が望ましい\n            - 説明変数を増やしたり減らしたりを繰り返して予測精度を推し量りながら特徴量を選択する手法\n                - 全ての変数に対してこの作業をすると、効率が悪い、結果の数値的な変化しか捉えられず根拠の乏しい対応\n            - 前提として統計量を用いた特徴量選択で相関関係の有無、強さを推定済みであることが望ましい\n                - **変数増加法(前方選択法)**\n                    - 前方選択とは、求められた特徴量重要度の**高い**ものから一つずつ特徴量をデータセットから**追加し**、予測精度を測る手法\n                - **変数現象法(後方削除法)**\n                    - 後方削除とは、求められた特徴量重要度の**低い**ものから一つずつ特徴量をデータセットから**取り除き**、予測精度を測る手法\n            - **メリット**\n                - どんな回帰分析手法とも組み合わせることができる\n                - 特徴量を追加・削除するシンプルな手法のため、特段、利用制限などはない\n            - **デメリット**\n                - 回帰モデルの構築を繰り返す回数次第で、処理コストが増大してしまう。\n        - **手法ベース特徴量選択法**\n            - アルゴリズムを用いて、正解ラベルと各特徴量の関係から、特徴量重要度(feature_importance)を求める方法\n                - **決定木ベース(ランダムフォレスト・勾配ブースティング)**\n                    - メリット：計算仮定を可視化できる\n                    - デメリット：特徴量同士で相関の強い変数が含まれる場合、重要度が低くでやすい\n                        - ランダムフォレスト：RandomForestClassifier\n                        - 勾配ブースティング：XGBoost,LightGBM\n                            - モデルの学習の際、その特徴量が使用された回数で推定する方法\n                            - その特徴量が使用される分岐から、どれくらいトレーニングデータに対して損失関数を小さくなったか(目的関数がどれだけ改善されたか)の幅で推定する方法\n                - **回帰(Lasso回帰、Ridge回帰、ElasticNet、ロジスティック回帰)**\n                    - 回帰係数(重み)から重要度を算出することが出来る。\n                    - メリット：回帰係数により計算しているため結果の要因分析をしやすい\n                    - デメリット：多重共線性のある特徴量に対しては重要度を正しく計測できないことがある\n                        - Lasso回帰\n                        - Ridge回帰\n                        - ElasticNet         \n       - 無用な特徴量は予測精度の低下や計算コストの増加を招く恐れがあるため、特徴量選択により、多数の特徴の中から目的とする予測に関連が強い特徴を取得選択する事\n        - **メリット**\n            - **目標変数の予測精度向上**\n                - 目的変数との関係性をより的確に説明する特徴量を無駄なく選択しモデルに学習させる事で、未知のデータに対しての予測誤差を最小にする\n                - 予測性能(汎化性能)の向上を測ることができるようになる\n            - **学習結果の解釈性の向上**\n                - 得られたモデルによる推論結果を人間が理解でき、根拠を説明可能であること。モデルのデバッグや精度改善の際に、説明変数のチェックが容易になる\n            - **計算効率の向上**\n                - 無駄なデータの学習コストを減らす事で、処理時間を減らすことがある。\n- **参考サイト**\n- [**特徴量エンジニアリング**](https://qiita.com/tk-tatsuro/items/f27c012e0cb95a5f51d2)\n- [**特徴量選択**](https://qiita.com/tk-tatsuro/items/759acd001d4fbb8ef6c5)\n- [**主成分分析**](https://qiita.com/tk-tatsuro/items/705c865009bc7ccb6b04)\n- [**相互作用特徴量**](https://qiita.com/tk-tatsuro/items/49a886cf3290a512aacf)\n- [**特徴選択**](https://qiita.com/tk-tatsuro/items/759acd001d4fbb8ef6c5)\n- [**欠損データ**](https://qiita.com/tk-tatsuro/items/3aaaf7139de19c0d8555)","metadata":{}},{"cell_type":"markdown","source":"## **2.1. モデルのデータ投入準備(訓練セット/検証セット/テストセット)**","metadata":{}},{"cell_type":"markdown","source":"***\n## **3. モデル**\n- **時系列データ系列の数理モデル(アルゴリズム)**\n    - **Simple Exponential Smoothing model (単純指数平滑化法モデル)**\n    - **Holt’s Linear Smoothing model (Holtの線形指数平滑法モデル)**\n    - **Holt-Winter’s Seasonal Smoothing model (ホルト-ウィンターズ法モデル)**\n    - **ARIMA**\n    - **SARIMA**\n    - **SARIMAX**\n- **テーブルデータ系列の数理モデル(アルゴリズム)**\n    - テーブルデータ系の数理モデル(アルゴリズム)を使い、**時系列予測モデル**を作るには、**時系列特徴量**を生成することで、対応できる\n    - **テーブルデータモデルを使い、時系列特徴量を作成する**\n        - **線形回帰モデル(単回帰、重回帰、etc...)**\n        - **正則化回帰モデル(Ridge回、Lasso回帰、ElasticNet、etc...)**\n            - **正則化項付き線形回帰モデル（Ridge回帰、Lasso回帰、Elastic net回帰など）** で **時系列予測モデルを構築**\n                - 正則化項付き線形回帰モデル\n                    - 通常の線形回帰モデルは、**最小二乗法**という方法で回帰式の**回帰係数を求める(定数含む)**\n                        - $y_{i}=\\beta_{0}+\\displaystyle\\sum_{j=1}^{p}\\beta_{j}x_{ij}$\n                        - yが目的変数で、xが説明変数。pが説明変数の数で、nがデータ数(データセットの行数)\n                            - 何かしら回帰係数(定数含む)を設定すると、何かしらの線形回帰式が出来上がる。何かしらの線形回帰式が出来上がれば、yの予測値を求めれる\n                            - **予測値と実測値の差を二乗した値を最小化**するような**回帰係数(定数含む)**を求める方法が、**最小二乗法**\n                                - $\\hat{\\beta}=\\underset{\\beta}{\\operatorname{argmin}}\\lbrace \\displaystyle\\sum_{i=1}^{n}(\\beta_{0}+\\displaystyle\\sum_{j=1}^{p}\\beta_{j}x_{ij}-y_{j})^{2} \\rbrace$\n                            - **最小二乗法にある制約(正則化項)**を付け加えた、**制約(正則化項)付き最小二乗法**という方法で回帰式の回帰係数(定数含む)を求めるのが、**正則化付き線形回帰モデル**\n                                - 通常線形回帰モデルと比べ、求めた回帰係数(定数含む)が0の方向に縮小(絶対値が小さくなる)\n                                - この**制約(正則化項)付き最小二乗法**という方法で回帰式の回帰係数(定数含む)を求めるのが、**正則化項付き線形回帰モデル**\n                                - この**制約(正則化項)**により、説明変数同士の相関が高いために起こる不具合や過学習などの影響を緩和する。\n                - **制約(正則化項)**、過学習などの影響を緩和\n                    - **どのような制約(正則化項)を加えるかで、変わる**\n                        - **Ridge回帰**\n                            - 線形回帰モデルの回帰係数の**二乗和を正則化項として加えた推定法**\n                            - ハイパーパラメータとして正則項の重み(正則化ハイパーパラメータ)\n                            - **線形回帰モデルの回帰係数の二乗和を正則化項として加えた推定法**\n                            - **ハイパーパラメータ**として、**正則化項の重み（正則化パラメータ）を設定**する必要\n                                - $\\hat{\\beta}_{ridge}=\\underset{\\beta}{\\operatorname{argmin}}\\lbrace \\displaystyle\\sum_{i=1}^{n}(\\beta_{0}+\\displaystyle\\sum_{j=1}^{p}\\beta_{j}x_{ij}-y_{j})^{2}+\\lambda \\displaystyle\\sum_{j=1}^{p}\\beta_{j}^{2} \\rbrace$\n                        - **Lasso回帰**\n                            - 線形回帰モデルの回帰係数の**絶対値の和を正則化項として加えた推定法**\n                            - ハイパーパラメータとして正則項の重み(正則化ハイパーパラメータ)\n                            - **Ridge回帰と異なり**、**回帰係数を0にするため変数選択を自動で実施**\n                            - **線形回帰モデルの回帰係数の絶対値の和を正則化項として加えた推定法**\n                            - **ハイパーパラメータ**として、**正則化項の重み（正則化パラメータ）を設定**する必要\n                                - $\\hat{\\beta}_{Lasso}=\\underset{\\beta}{\\operatorname{argmin}}\\lbrace \\displaystyle\\sum_{i=1}^{n}(\\beta_{0}+\\displaystyle\\sum_{j=1}^{p}\\beta_{j}x_{ij}-y_{j})^{2}+\\lambda \\displaystyle\\sum_{j=1}^{p}|\\beta_{j}| \\rbrace$\n                        - **ElasticNet回帰**\n                            - **Ridge回帰**と**Lasso回帰**の**混合モデル**\n                            - ハイパーパラメータの指定でRidgeかLassoかに寄せれる。\n                            - **ハイパーパラメータ**として、その**割合を設定**\n                            - **ハイパーパラメータ**として、**正則化項の重み（正則化パラメータ）を設定**\n                                - $\\hat{\\beta}_{elasticnet}=\\underset{\\beta}{\\operatorname{argmin}}\\lbrace \\displaystyle\\sum_{i=1}^{n}(\\beta_{0}+\\displaystyle\\sum_{j=1}^{p}\\beta_{j}x_{ij}-y_{j})^{2}+\\lambda \\displaystyle\\sum_{j=1}^{p}(\\alpha|\\beta_{j}|+(1+\\alpha)\\beta_{j}^{2}) \\rbrace$\n                            \n        - **一般線形モデル(GLMM)**\n        - **一般加法モデル(GAM)**\n        - **階層線形モデル、マルチレベルモデル、一般混合モデル**\n        - **決定木**\n            - **SVMと同様に、決定木**は、分類と回帰の両方のタスクを実行でき、多出力タスクさえこなせる柔軟性の高いアルゴリズム\n            - 決定木は単体でも強力で、複雑なデータセットに適合可能。\n                - ランダムフォレストの基礎構成要素\n            - **ルートノードから葉ノードへの分岐によって予測分類していく**\n            - 決定木には多くの特徴があるのが、その１つはデータの準備がほとんど不要。**特にスケーリングやセンタリングを必要としない**\n                - **CART訓練アルゴリズム(Classification and Regression Tree)**\n                    - scikit-learnはCARTを使用。アルゴリズムが最小化しようとするコスト関数\n                        - $J(k,t_{k})=\\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$\n                        - $\\left\\{\\begin{matrix} G_{left/right}は、左右のサブセットの不純度 \\\\ m_{left/right}は、左右のサブセットのインスタンス数\\end{matrix}\\right.$\n                - **NP完全問題**\n                    - 最適な木をみつけるという問題はNP完全問題\n                        - Pは多項式時間で解決できる問題、NPは多項式時間で解を検証できる問題の集合\n                        - **NP困難問題**は、NP問題から多項式時間還元可能な問題。\n                        - **NP完全問題**は、NPであり、NP困難問題。\n                        - **P=NP**かどうかは、数学の重要な未解決問題であり、$N\\neq P$なら(そうらしく見える)**NP完全問題**には多項式アルゴリズムは見つからない。\n                            - 量子アルゴリズムであれば、見つけられる可能性はある\n                - **計算量**\n                    - $O(exp(m))$なので、ごく小規模な訓練セットでも、手に負えない程の計算量になる。\n                    - **ある程度よい解**で満足しなければならないのはその為\n                - **(分類)ジニ不純度orエントロピー**\n                    - ジニ不純度とエントロピーのどちらを使ってもよい、大差はない。\n                    - **エントロピー**\n                        - 全てのメッセージが同じなら、エントロピーは0\n                        - 集合が１つのクラスに属するインスタンス(要素)だけから構成されている場合、集合のエントロピーは0\n                        - $H_{i}=-\\displaystyle\\sum_{\\substack{k=1 \\\\ p_{i,k} \\neq 0}}^{n} p_{i,k} \\log_{2}(p_{i,k})$\n                    - **ジニ係数**\n                        - $G_{i}=1-\\displaystyle\\sum_{k=1}^{n}p_{i,k^2}$\n                        - $p_{i,k}$はi番目のノードの訓練インスタンス数のなかのクラスkのインスタンスの割合\n                        - scikit-learnは、二分木しか作成しないCARTアルゴリズムを使用しており、**葉ノード以外のノードには、必ず２つの子ノードがある**イエスノーの２つしか答えが無い\n                        - ID3などの他のアルゴリズムは、決定木のなかに３つ以上の子があるノードを作ることができる。\n                    - **デフォルトでは、ジニ不純度**\n                        - **ハイパーパラメータ**\n                            - $criterion=\"entropy\"$\n                - **うまく汎化しない場合**\n                    - **PCAを使用すれば、訓練データの向きをよい方向に変えられる事が多いので、ある程度劣悪汎化軽減が可能**\n                - **決定木の問題:不安定性**\n                    - 決定木の最大の問題は、訓練セットの小さな変化に敏感すぎること。\n                    - ランダムフォレストは多数の木の予測を平均するので、不安定性の問題を軽減できる\n                - **決定木の過少学習・過学習**\n                    - 過少学習  :　決定木は訓練データのスケーリングやセンタリングの影響を受けない。 \n                    - 過学習　　:  max_depthを下げることによって、モデルに制約を加え正則化する\n        - **ランダムフォレスト**\n            - **複数の決定木の結果を束ね予測結果とする**\n                - **結果を束ねると**は、平均値、多数決など\n            - **複数の決定木**\n                - 同じデータセットで作るわけではなく、それぞれ別のデータセットで作成する。\n                - **元のデータセット**から複数のデータセットをランダムに作る。\n                    - そのデータセットごとに決定木を作る。\n                    - ランダムなデータセットで、複数の決定木を作成する為、ランダムフォレストと呼ばれる。\n                - 元のデータセットから複数のデータセットの作り方は意外とシンプルで、元のデータセットの行(ケース)と列(変数)をランダムに選ぶ\n                    - **行(ケース)**：元のデータセットから復元抽出(ブートストラップ)でランダムにデータを抽出しデータセットを複数作る\n                    - **列(変数)**　：そのデータセットの説明変数(特徴量)をランダムに選択し、新たなデータセットとする\n        - **ブースティングモデル(AdaBoost、XGBoost、LightGBM、etc...)**\n            - **XGBoost**\n                - **ランダムフォレスト**と似た数理モデル、**ブースティングモデル**\n                - 基本的には**アンサンブル学習**であり、複数の学習器を構築し１つの学習器に集結させる手法\n                - **ランダムフォレストとXGBoost**の違い\n                    - **並列アンサンブル** : ランダムフォレスト\n                    - **直列アンサンブル** : XGBoost\n        - **NeuralNetwork**","metadata":{}},{"cell_type":"markdown","source":"## **3.1 モデルの考察 (時系列の将来を予測するモデルの作成 or 分類器の作成)**\n- **Baselineを先に作成してからモデルの考察をする**\n    - **乗法モデル**\n        - 変動の積からなるモデル\n            - 観測値 = Trend(傾向変動) * Cyclical(循環変動) * Seasonal(季節変動) * Irregular(不規則変動)\n                - 乗法モデルでは、季節性の影響は時間の経過とともに増加する。\n    - **加法モデル**\n        - 変動の和からなるモデル\n            - 観測値 = Trend(傾向変動) + Cyclical(循環変動) + Seasonal(季節変動) + Irregular(不規則変動)\n                - 季節性による予測変動は年間を通して一定。実際には変動が増加する。\n                    \n- **自己回帰モデル**\n    - 過去の値の重み付き平均\n        - いろいろな状況に対応した予測できる\n        - ARモデル　：　時系列の現在の値を過去の値で表現する\n        - ノイズ　：　予測誤差　：　平均$0$, 分散$\\sigma^{2}$ : 正規分布\n        - 次数$m$ : 代えると予測結果が変わる、予測精度がよくなったりする\n        - 次数選択(評価基準) ： AIC, BIC\n            - パラメータ推定\n                - Yule-Walker法、最小二乗法、Burg法、最尤法、ベイズ法\n                - Optunaを使用してハイパーパラメータの自動調整\n            - 次数選択\n                - 予測誤差の意味で最適な次数を選択する為に、情報量規定\n                - $AIC_{m} = N(\\log 2 \\pi \\hat{\\sigma}^{2} +2(m+1))$ : 最初にする$m$を探す。但し、$\\hat{\\sigma}^2$は予測誤差分散の推定値\n    \n- **ナイーブ予測**\n    - ランダムウォーク型\n        - 前期と同じ値にする\n        - $y_{n|n-1} = y_{n-1}$\n    \n- **回帰型予測**\n    - マルコフ型\n        - 前期の値に適当な係数を掛けたもの\n        - $y_{n|n-1} = \\alpha_{1}y_{n-1}$\n    \n- **Prophet**\n    - [Optuna チュートリアル](https://optuna.readthedocs.io/en/stable/tutorial/index.html#key-features)\n    - Facebookで一般的に遭遇するビジネスケース向けに構築されているが、他のビジネスでも遭遇する。\n        - 【応用可能例】\n            - 十分な履歴データを含む時間別、日別、または週別のデータ\n            - 人間の行動に関連する複数の季節性パターン(曜日、季節)\n            - 不定期の重要な祝日(感謝祭、旧正月)\n            - 妥当な量の欠損データ\n            - 過去の傾向の変化\n            - 飽和を伴う非線形の成長傾向(容量制限など)\n                - ユーザーは、製品導入による傾向変化点や容量による傾向飽和値などの既知のパラメータを導入することで、モデル構築プロセス中に直観的に介入できる。\n                - Stanを最適化エンジンとして使用。モデルに適合させ、不確実性区間を計算、Stanはデフォルトで最大アプリオリ(MAP)最適化を実行するが、サンプリングを要求できる場合に限る\n                \n- **`Neural Prophet`**\n    - [ライブラリ](https://neuralprophet.com/contents.html)\n        - NeuralProphet は非常に強力だがパラメータの調整も困難。\n        - 理由は、NeuralProphet の規則がパラメーターの分布の形状を調整し、 NeuralProphet が分布から最適なパラメーターを見つけるため。\n        - レギュレーションの微調整は全体の分布の調整であり、ほぼ「全身を引っ張る」とみなすことができる。\n        - そのため、**パラメータの自動チューニングを行う必要があります。**\n        - ここでは**パラメータの最適化に Optuna を使う**のがよい。 **Optuna は GridSearch に比べて最適化速度が速い**\n    - **ハイパーパラメータ**\n        - **Trend**\n            - **growth** : \n                - トレンドが線形であるかトレンドがないかに関係なく、NeuralProphet は不連続な新しい成長方法を提供し、\n                - トレンドが壊れたりジャンプしたりできるようにします。つまり、より柔軟になります (そしてオーバーフィットします)。\n            - **changepoints** : \n                - 変更点のリスト。通常、NeuralProphet が自分で見つけられるようにします。\n            - **n_changepoints** : \n                - 変化点の数\n            - **changepoints_range** :\n                - 変化点検出の範囲。デフォルトは 0.8 で、一般に影響が大きくなります。\n            - **trend_reg**：\n                - トレンドアイテムの規則性（制限）、つまりトレンドアイテムの柔軟性 記事冒頭で述べた規制の一つで、影響力が大きい。\n        - **Season**\n            - **yearly_seasonality/ week_seasonality /daily_seasonality** :\n                - これら3つのサイクルの季節成分を自動的に検出するかどうか、NeuralProphetはデータセットの長さと解像度によって自動的に設定する。\n                - 私は通常、これを無効にして自分でピリオドを追加し、そのパラメータを調整します：フーリエ級数\n            - **seasonality_mode**:\n                - 加法的または乗法的、一般的には乗法的な方がより柔軟性がある。\n            - **seasonality_reg**:\n                - 季節項に対する規則的な制限、これも大きな影響を与える\n        - **AR関連**\n            - **n_lags**:\n                - ラグの順序。複数ステップの予測 (n_forecasts> 1) の場合、n_forecasts より大きくする必要があります\n            - **ar_reg**:\n                - ARアイテムのレギュラー\n        - **ニューラルネットワーク構造**\n            - **num_hidden_layers**:\n                - レイヤーの数、ハイパーパラメーターでもあります\n            - **d_hidden**:\n                - レイヤーあたりのユニット数\n        - **フィッティング パラメータ**\n            - **learning_rate**:\n                - 学習率\n            - **epochs**:\n                - エポック フィッティング時間\n            - **batch_size**:\n                - 各フィッティングのバッチ\n                \n- **`平滑化法`**\n    - **`Simple Exponential Smoothing model（単純指数平滑化法モデル）の特徴`**\n        - トレンド成分：**なし**\n        - 季節成分：**なし**\n        - **トレンド成分も季節成分もなさそうな、時系列データをモデル化する手法**\n            - 数式で表現すると、時系列データ $y_{1},y_{2},・・・,y_{n}$ \n                - $\\hat{y}_{t} : 予測値$\n                - $\\hat{y}_{t}=\\alpha y_{t}+(1+\\alpha)\\hat{y}_{t-1}$\n    - **`Holt’s Linear Smoothing model（Holtの線形指数平滑法モデル）の特徴`**\n        - トレンド成分：**あり**\n        - 季節成分：**なし**\n        - **トレンド成分はありそうだが、季節成分がなさそうな時系列データをモデル化する手法**\n            - $\\hat{y}_{t+h} : h期先の予測値$\n            - 数式で表現すると、時系列データ$y_{1},y_{2},・・・,y_{n}$に対し、以下\n                - $\\hat{t+h}=l_{t}+hb_{t}$\n                - $l_{t}=\\alpha y_{t}+(1-\\alpha)(l_{t-1}+b_{t-1})$\n                - $b_{t}=\\beta(l_{t}-l_{t-1})+(1-\\beta)b_{t-1}$\n    - **`Holt-Winter’s Seasonal Smoothing model（ホルト-ウィンターズ法モデル）の特徴`**\n        - トレンド成分：**あり（なしにもできる）**\n        - 季節成分：**あり（なしにもできる）**\n        - **Holt-Winter’s Seasonal Smoothing model（ホルト-ウィンターズ法モデル）は、トレンド成分を考慮したりしなかったり、季節成分を考慮したりしなかったりと、非常に使い勝手がいい**\n            - $\\hat{y}_{t+h}$は$h期先の予測値$\n            - $h_{m}^{+}=\\lfloor (h-1) \\mod m\\rfloor + 1$\n            - 数式で表現すると、時系列データ$y_{1},y_{2},・・・,y_{n}$に対し、以下\n                - $\\hat{y}_{t+h}=l_{t}+hb_{t}+S_{t-m+h_{m}^{+}}$\n                - $l_{t}=\\alpha(y_{t}-S_{t-m})+(1-\\alpha)(l_{t-1}+b_{t-1})$\n                - $b_{t}=\\beta(l_{t}-l_{t-1})+(1-\\beta)b_{t-1}$\n                - $S_{t}=\\gamma(y_{t}-l_{t}-b_{t-1})+(1-\\gamma)S_{t-m}$\n- **`ARIMA`**\n    - ARIMAはAuto-Regressive Integrated Moving Average Model\n    - **このモデルは、次の3つの成分から構成**\n        - **AR (Auto-Regressive) component：自己回帰成分**\n        - **I (Integrated) component：和分成分**\n        - **MA (Moving Average) component：移動平均成分**\n    - 時系列データは、上昇傾向や下降傾向などのトレンドが見られ、明らかに定常でない\n        - **I (Integrated) 成分で定常化を目指**\n            - **モデルを構築するというよりも、前処理と表現したほうがいいでしょう。この場合の前処理とは、過去との差分を取り階差系列を作ること**\n            - 今日の売上と昨日の売上の差分を求め、その差分を新たな時系列データとする、ということです。その新たなデータを階差系列と呼ぶ。\n            - 上昇傾向や下降傾向などのトレンドがなくなるまで、この前処理を繰り返します。多くの場合、1,２回で十分です。この繰り返し数を、差分の階数dで表現する。\n                - **時系列データは、過去の値に依存する場合があります。自己相関があるということ**\n        - 例えば、今日の売上が1日前（1期前）と似たような値を取るケースです。AR (Auto-Regressive)で、自己相関をモデル化\n            - AR (Auto-Regressive)でモデル化するとき、**何期前（p）までのデータを用いるか**を考えなくてはなりません。それを**自己回帰パラメータp**\n                - 時系列データは、**AR (Auto-Regressive)だけで自己相関を十分にモデル化できない**\n                - 自己相関は、**MA (Moving Average)でもモデル化することができます。** どこまでの過去を考慮するのかを考える。\n                - それを移動平均パラメータqと言います。移動平均とは、残差の移動平均ですので注意\n        - **3つのパラメータ**\n            - **自己回帰パラメータp**\n            - **差分の階数d**\n            - **移動平均パラメータq**\n                - パラメータ設定したかを明示的に示すために、**ARIMA(p,d,q)**という形式で表現\n            - ARIMAモデルでは、これらのパラメータは非負の整数値（0,1,2,…）の値を取ります。パラメータの値を「次数」という表現。\n                - 問題は、この(p,d,q)の次数の値をどう設定するのか\n\n- **`SeasonalARIMA : ARIMAモデルを拡張したSARIMA`**\n    - ARIMAは、季節性を効果的に捉えることができないという問題があります。この問題を解決するのが、SARIMA（Seasonal ARIMA）モデルです。\n    - SARIMAモデルのパラメータは、以下のようになります。\n        - 非季節性パラメータ（p,d,q）\n        - 季節性パラメータ (P, D, Q, m)\n    - 非季節性パラメータ（p,d,q）は、ARIMAモデルのパラメータと同じです。\n        - p：ARIMA の AR componentの次数（自己回帰パラメータ）と同じ\n        - d：ARIMA の I componentの次数（差分の階数）と同じ\n        - q：ARIMA の MA componentの次数（移動平均パラメータ）と同じ\n    - ARIMAモデルと大きく異なるのは、以下の季節性パラメータ (P, D, Q, m)の存在です。\n        - m (Seasonal Period)：季節性の周期\n        - P (Seasonal AR component)：季節性の AR componentの次数\n        - D (Seasonal I component)：季節性の I componentの次数\n        - Q (Seasonal MA Component)：季節性のMA componentの次数\n    - m は季節性の周期です。例えば、月単位の時系列データに対し12ヶ月周期がある場合、m=12 となります。\n    - Pは、季節性の AR componentの次数です。例えば、月単位の時系列データに対しm=12かつP=2の場合、12ヶ月前と24ヶ月前のデータの値を考慮するということです。\n    - Dは、季節性の I componentの次数です。例えば、月単位の時系列データに対しm=12かつD=1の場合、12ヶ月前とデータとの差分をとるということです。\n    - Qは、季節性のMA componentの次数です。例えば、月単位の時系列データに対しm=12かつQ=2の場合、12ヶ月前と24ヶ月前の残差を考慮するということです。\n        - どのようにパラメータ設定したかを明示的に示すために、SARIMA(p,d,q)(P, D, Q, m)もしくはSARIMA( p,d,q )( P,D,Q )mという形式で表現\n\n- **`説明変数X付きSARIMAXモデル`**\n    - SARIMAモデルは、過去の自分（ラグデータ）を使ってモデルを構築します。説明変数Xを導入しモデルに組み込めないだろうか、ということでSARIMAX\n        - このXは説明変数X\n    - 要するに、SARIMAXは、説明変数X付きのSARIMAモデルです。\n    - つまり、ARIMA系のモデルとは、以下のモデル全てを含む\n        - ARモデル：SARIMA( p,0,0 )( 0,0,0 )0\n        - MAモデル：SARIMA( 0,0,q )( 0,0,0 )0\n        - ARMAモデル：SARIMA( p,0,q )( 0,0,0 )0\n        - ARIMAモデル：SARIMA( p,d,q )( 0,0,0 )0\n        - SARIMAモデル：SARIMA( p,d,q )( P,D,Q )m\n        - SARIMAXモデル：SARIMAX( p,d,q )( P,D,Q )m\n    - パラメータ(p,d,q) (P,D,Q)m の設定の仕方\n        - ARIMA系のモデルのパラメータ(p,d,q) (P,D,Q)mの次数をどう設定するのかは、非常に大きな問題です。\n            - 通常は、周期mを事前に設定した上で、残りのパラメータ(p,d,q)と(P,D,Q)を検討\n                - **3つの方法**\n                    - **手動構築 ⇒**\n                        - 手動構築とは、**ADF検定などを使い差分の次数を検討し決めます。**\n                        - **その次数で階差系列を求め**、**その階差系列に対し自己相関 (ACF)と偏自己相関 (PACF)のコレログラムを表示**し、**ARとMAの次数を検討**\n                            - ⇒　手動構築の場合、コレログラムが次数を決める上でのポイント\n                    - |      |  Acf<br>ラグ1,2,3,... | Pacf<br>ラグ1,2,3,... | \n| ---- | ---- | ----|\n|  AR(p)  |  指数関数的に減少  | ラグp1の後は0 |\n|  MA(q)  |  ラグqの後は0  |   指数関数的に減少  |\n|  AR(p)+MA(q) |  指数関数的に減少  |  指数関数的に減少  |\n                    - |      |  Acf<br>ラグ1×m,2×m,3×m,… | Pacf<br>ラグ1×m,2×m,3×m,… | \n| ---- | ---- | ----|\n|  SAR(P)  |  指数関数的に減少  | ラグP×m1の後は0 |\n|  SMA(Q)  |  ラグQ×m1の後は0  |   指数関数的に減少  |\n|  SAR(P)+SMA(Q) |  指数関数的に減少  |  指数関数的に減少  |\n    - **`SARIMA自動構築(機械学習的なモデル構築)`**\n    - よくあるのは手動構築と自動構築のハイブリッド\n        - **パターン１**：**自動構築で次数を求め**、次に、**その次数を参考に手動で次数を検討し決める**\n        - **パターン２**：**階差の次数のみ検討**し、次に、**その階差の次数以外を自動構築で決める**\n        - conda install pmdarima\n        - pip install pmdarima\n            - **ライブラリーpmdarima**に、時系列データを扱う上で**便利な道具**がいくつか揃っている\n                - **単位根検定やコレログラム**\n                - **データセットの分割（学習データとテストデータ**\n                - **時系列CV（クロスバリデーション）**\n                - **BoxCox変換などなど。**\n","metadata":{}},{"cell_type":"markdown","source":"## **3.2 モデル考察(ブレンディング・スタッキング)**\n- **Baselineを先に作成してからモデルの考察をする**\n- **モデルブレンディング(ModelBlending)**\n    - **ブレンディング/スタッキング**\n        - アンサンブル学習\n            - **平均法(Averaging)**：複数のモデルによる各予測結果を「平均」する方法(回帰の場合)\n            - **投票法(Voting)**：複数のモデルによる各予測結果で「多数決」(分類)\n            - **ブレンディング(Blending)**：\n                - 【Level0】：複数のモデルによる各予測結果を、\n                - 【Level1】：線形モデル(回帰：線形回帰、分類：ロジスティック回帰)などで「ブレンド」する\n                \n            - **スタッキング(Stacking)**：\n                - 【Level0】：複数のモデルによる各予測結果を、\n                - 【Level1】：複数の新たなモデルを構築し、その予測結果を、\n                - 【Level2】：線形モデル(回帰：線形回帰、分類：ロジスティック回帰)などで「ブレンド」する\n                    - 【Level0~2】の3層を「スタッキング」する方法\n                    \n        - **ブレンディング/スタッキング:model**\n            - **ベースモデル(BaseModel)**：【Level0】で作成されるモデル\n            - **メタモデル(MetaModel)**： 【Level1】【Level2】で作成したモデル\n                - 各予測結果を用いて訓練された新たな上位モデル\n            \n            - **【Level0】**：ベースとなる、複数(例５)の機械学習モデルを作成。**ベースモデル**\n                - 【Level_0】model_1 : XGBoost(ターゲットエンコーディングのバージョン)\n                - 【Level_0】model_2 : ランダムフォレスト\n                - 【Level_0】model_3 : XGBoost(標準化バージョン)\n                - 【Level_0】model_4 : XGBoost(多項式と交互作用の特徴量)\n                - 【Level_0】model_5 : XGBoost(ワンホット・エンコーディングのバージョン)\n                    - etc...model\n                        - CatBoost/HistGradientBoostingRegressor : model\n            - **【Level0⇒1】ベースモデルによる予測結果から新たなデータセット(訓練/検証/テスト)を作成\n                - 予測結果から新たなデータセットを作る\n                    - 【Level1】のメタモデルを作成する前に、**モデルに入力するためのデータセット**を作る必要がある\n                        - 【Level0】で作成したベースモデルの予測結果から作成する。\n                            - モデル１～５の検証データをマージし、**新しい１つの訓練&検証データ**を作成する\n                            - モデル１～５のテストデータをマージし、**１つの新しいテストデータを作成**\n                                - ［id］列の値（ID）を手掛かりに［pred_1］～［pred_5］列をマージしていくことで、「フォールド（＝分割）を一致させる」ことを実現\n            - **【Level1】*: 最終的な予測を行うための、１つの**メタモデル**\n                - **ブレンディング**がここまで\n            - **【Level1⇒2】*：【Level1】メタモデルにする予測結果から新しいデータセット(訓練/検証/テスト)\n                - モデルに入力するためのデータセットを作る。データセットは、【Level1】のメタモデルによる予測結果から作成する。\n                    - 【Level0⇒1】の作成を適用する\n            - **【Level2】*: 最終的な予測を行うための、１つのメタモデルを作成\n                - 最後の**メタモデル**を作成する。","metadata":{}},{"cell_type":"markdown","source":"***\n## **3.3 MMM:Marketing Mix Modeling(マーケティングミックスモデリング)**\n- **MMM:Marketing Mix Modeling**\n- この方法は、**本当に売り上げに貢献している広告は、どのドメインの広告か？**を明瞭にする方法。\n    - 伸びる余地のある別の広告にアロケーションする等の判断に役立つ\n- **売上と広告媒体との関係性をモデリングし**、どの広告媒体が売り上げにどれだけの貢献、寄与しているかを分析する\n- **モデルが線形である為に、広告費用を増やす事で売上をいくらでも増やす事が可能という状態が発生する。これはトリックであり現実性はない。**\n    - **これは、対象広告のリーチに対して、上限という物が存在するので、この結果を新たなデータセットとし、トレンド転換点を観察すれば、リーチ上限を観察可能になる**\n    - **仮に売上を最大化する目的で、係数が最高のチャネルへ全て投資を投入しようとも、他の媒体広告を完全に放棄する事は、認知度等のリスクを考えても正しい選択とは言えない。**\n- **MMM**は、単純なこれまでの効果を分析するのではなく、今後の推移はどうなるのかという予測分析であり、**扱うデータは時系列データ**\n    - メディアインプレッション、コントロール変数に分かれる\n        - **メディアインプレッション**\n            - TVCM,看板広告,Web広告,車内広告,検索連動広告,自然検索,AFC,DPS,Mail\n        - **コントロール変数**\n            - 季節要因,自然災害,ウイルスパンデミック,マクロ経済情報(景気),競合他社商品売上\n- **MMMアーキテクチャ**\n    <details><summary>サンプルコード</summary>\n    \n    ```python\n    \n    ControlModel = Sales + Control(MacroEconomy, Discount, Holiday, Seasonality)\n    Media_inplession = [Direct_Mail, TV, Radio, SEM, Online_Display, Social_Media]\n    \n    AdStock = Media_inplession\n    \n    Marketing_Mix_Model = ControlModel + Adstock + Sales\n    \n    Media_Contribution.plot(Marketing_Mix_Model)\n    \n    DiminishingReturnModel(Media_Spending + Media_Contribution)\n    \n    ROAS(DiminishingReturnModel)\n    MarginalROAS(DIminishingReturnModel)\n    ```\n    </details>\n\n- **利用するデータセット例**\n    - **Week**：週\n    - **Sales**：売上\n    - **TVCM**：TV CMのコスト\n    - **Newspaper**：新聞の折り込みチラシのコスト\n    - **Web**：Web広告のコスト\n- **得られる結果**\n    - **売上貢献度**\n        - **どの広告媒体が、どれだけ売上に貢献したのかを可視化**\n        - **売り上げ貢献度を計算：数理モデルを構築：加法モデル**\n            - **売上 = f(テレビ) + g(ニュースペーパー) + h(Webバナー) + ベース**\n            - **$Sales = \\beta_{0} + \\beta_{TVCM} × TVCM + \\beta_{Newspaper} × Newspaper + \\beta_{Web} × Web + \\epsilon$**\n            - ⇒　**予測モデルに使用**\n    - **ROI =（売上貢献度 － コスト）÷ コスト**\n        - **投資収益率(ROI)**\n            - どのチャネルが最適であったのかを判断可能・\n            - **channel_ROI = Sales from the channel / channel spendings**\n                - TV_ROIの場合、テレビからの売り上げ / テレビでの支出によりTVに費やした広告費用によってどの位の金額がペイ出来たかがわかる。\n        - **広告媒体のコストデータがあれば、ROIを可視化**\n- **予測モデル**\n    - **ランダムフォレスト、勾配ブースティング、FFNNなどのモデルは、分解を取得できないため、適さない。**\n        - **通常線形、重回帰モデルを使用する**\n        - **ベイズモデルや状態空間モデルを取り入れたアプローチも可能**\n        - **ロジスティックを使用して、パラメータに事前分布を設定し、線形モデルでアプローチ**\n        - **階層型ベイズ**\n- **ｋ分割交差検証**\n    - **時系列データを扱う為、通常のK分割交差検証は使用できない。**\n        - **より合理的な方法でScikit-learnのTimeSeriesSplitを使用して時系列の交差検証を実行する**\n- **MMM構築パイプライン**\n    - 1. **売上を目的変数**、TVCM・Newspaper・WebCostを説明変数にモデルを構築\n    - 2. **構築したモデルを使用し**、**媒体別の売り上げ貢献度を求める**\n    - 3. 売上貢献度を足し合わせると**売上予測値**になる。この状態では実測値と乖離しているので、**補正係数を計算し、補正済みの売上貢献度を計算する**\n    - 4. **補正済みの売り上げ貢献度を使用**し、**媒体別のROIを計算**    \n- **MMM実行後**\n    - 実行後は、広告の最適化を行う為に、予算制約の元で最大化する解を得る\n        - **例**\n            - $f(x) = \\beta_{0} + \\beta_{1}  * \\log (x_{TV} + 1)+ \\beta_{2} * \\log(\\lambda x_{TV-1} + 1)+\\beta_{3}*\\log(\\lambda^{2}x_{TV-2}+1)+ ...+\\beta_{n} * \\log (x_{n}+1)$\n\n### **`AdStockモデル(アドストックモデル)`**\n- $adstock(x_{t-L+1,m,...},x_{t,m};w_{m},L)=\\frac{\\sum_{t=0}^{L-1}w_{m}(l)x_{t-l},m}{\\sum_{l=0}^{L-1}w_{m}(l)}$\n    - $w_{m}l$ ： 非負ウェイト、ラグ期間関数\n    - $L$ : ラグ効果の最大期間：論文L=13\n    - $x_{t,m}$ : ある期間のあるメディアの支出\n- **AdStock**を考慮しないモデルは**Baselineモデル**として規準にする\n- アドストック（Ad Stock）を考慮するとは、**飽和モデル(収穫逓減)**　と　**残存モデルを考慮する**\n    - **ラグ効果・残差効果・キャリーオーヴァー効果**\n    - その日だけに効果があるのではなく、次の日以降もその効果が続くということ\n- **飽和効果＋ラグ効果 = AdStock(アドストック)**\n    - 効果が遅れてくる、効果が後々まで残っている\n        - **ドメイン知**\n            - 高額商品、広告などに触れた瞬間購入ではなく、数日～の検討によって購入\n            - 消費財等の安価商品、広告に触れるうちに必要になってくる、購入したくなる場合もある。\n- **AdStockモデルでの変数の相関の対処**\n    - 広告などのマーケティング変数は、**お互いに強い相関を示すことがある**。なぜならば、同じ時期にキャンペーンという名目で広告などを投入したりする\n        - これが理由により、線形回帰モデルにとってやっかいな現象が起こることがある\n        - **対策**\n            - **正則化項付き線形回帰モデルを使用する。**\n                - **Ridge回帰、Lasso回帰、ElasticNet** ⇒ 長項になるのでRidge,Lasso,ElasticNet項で説明\n            - **主成分分析回帰モデル(PCR)**\n                - **主成分分析回帰(PCR)** ⇒ 主成分分析回帰項で説明\n            - **PLS回帰**\n                - **部分的最小2乗回帰** ⇒ PLS項で説明\n- **AdStockモデルでの単純線形回帰モデルの問題点**\n    - ある広告にコストをかければかけるほど、**売上の上昇幅は鈍くなる**、経済学でいうところの**収穫逓減**が起こる\n    - 売上が飽和し、売上は飽和し、いくらコストをかけても売上が伸びなくなる。\n        - AdStockを考慮した線形回帰モデルで目的変数(売上)を予測するとは\n            - 広告などの説明変数のデータ（今回の例では、広告などのコスト）を**数値変換（変換器）**\n            - **線形回帰モデルの新たなインプットデータ**を作り、その**インプットデータを使い線形回帰モデルで目的変数である売上（Sales）を予測すること**\n                - **元の説明変数のデータを数値変換（変換器）** し、このデータを使い線形回帰モデルを構築する。**つまりパイプラインを構築する**\n- **AdStockを表現する関数の選定**\n    - **最適なAdStockを探索し、モデル構築する**、つまり飽和モデルとラグモデルの最適な組み合わせを探す\n    - **解釈**\n        - AdStockを表現する関数が、**違和感なく解釈可能なものかどうか**\n        - 解釈はドメイン(現場感)が重要\n    - **精度**\n        - 目的変数Y(例：売上、Sales)の**予測値と実測値が近い値になっているかどうか**というもの\n        - 機械的手法で検討する。最も精度の高いものを探索する\n- **AdStockモデルの最適な組み合わせ**\n    - **2種類のラグ効果モデルから関数を1つ**、**3種類の飽和モデルから関数を1つ選択**する\n    - **ラグモデル**\n    - **ラグ変換器**\n        - **定率減少型ラグモデル(Simple_CarryOver)**\n            - ピークが広告などの投入時で**徐々に低減するモデル**\n                - $x_{t}^{*}=\\displaystyle\\sum_{l=0}^{L-1}w_{t-l}・x_{t-l}, w_{t-l}=R^{l}$\n                - $x_{t}はt期の広告などの投入量で、x_{x}^{*}はt期とそれ以前までの広告などの効果の累積(残存・ラグ効果を加算蓄積)$\n                - **ハイパーパラメータ**\n                    - **L(length)**：効果の続く期間 * 当期は含まず\n                        - 期間とは、どのくらいまで考慮するか\n                    - **R(rate)**：減衰率\n        - **ピーク可変型ラグモデル(Peak_CarryOver)**\n            - ピークが広告などの投入時に**限らない**\n                - $x_{t}^{*}=\\frac{\\displaystyle\\sum_{l=0}^{L-1}w_{t-l}・x_{t-l}}{\\displaystyle\\sum_{l=0}^{L-1}w_{t-l}},w_{t-l}=R^{(l-P)^{2}}$\n                - $x_{t}はt期の広告などの投入量で、x_{x}^{*}はt期とそれ以前までの広告などの効果の累積(残存・ラグ効果を加算蓄積)$\n                - **ハイパーパラメータ**\n                    - **L(length)**：効果の続く期間　※当期含む\n                    - **P(peak)**：ピークの時期（広告などを打った日の場合は0、次期は1、など）\n                    - **R(rate)**：減衰率\n    - **飽和モデル**\n    - **飽和変換器**\n        - **指数型飽和(exp_Saturation)**\n            - $x_{t}^{**}=1-e^{-\\alpha x_{t}^{*}}$\n            - $x_{t}^{*}はt期とそれ以前までの広告などの効果の蓄積(残存・ラグ効果を加算蓄積)で、x_{t}^{**}は指数型飽和モデルで変化した後のt期の値$\n        - **ロジスティック型飽和モデル(logit_Saturation)**\n            - $x_{t}^{**}=\\frac{K}{1+be^{c(c_{t}^{*}-m)}}$\n        - **ゴンペルツ型飽和モデル(gom_Saturation)**\n            - $x_{t}^{**}=Kb^{e^{-c(x_{t}^{*}-m)}}$\n- **AdStockモデルのパイプライン作成の流れ**\n    - **入力(変数) ⇒ 変換器(残差・飽和：2つの変換器) ⇒　学習器(Model) ⇒ 出力**\n    - ⇒ 変数(入力) ⇒ ラグモデル ⇒ ラグモデル(出力) ⇒ 飽和モデル ⇒ 飽和モデル(出力) ⇒ モデル\n        - **変換器 : 【1】**\n            - **広告を打った時が効果ピーク、徐々に効果が一定の割合で減退していくモデル**\n            - **ラグ(キャリーオーバー)効果モデル**\n                - ラグ効果モデルを表現する数理モデルは複数ある。\n                - **シンプルな効果モデル：**\n                    - **広告などを打ったときが効果のピークで、徐々に効果が一定の割合で減退していくモデル**\n                        - $x_{t}^{*}=\\displaystyle\\sum_{l=0}^{L}w_{t-l}・x_{t-l}, w_{t-l}=R^{l}$\n                        - $x_{t}はt期の広告などの投入量で、x_{x}^{*}はt期とそれ以前までの広告などの効果の累積(残存・ラグ効果を加算蓄積)$\n                        - **ハイパーパラメータ**\n                            - **L(length)**：効果の続く期間 * 当期は含まず\n                                - 期間とは、どのくらいまで考慮するか\n                            - **R(rate)**：減衰率\n            - **飽和モデル**\n                - 飽和（収穫逓減）を表現する数理モデルは数存在する。\n                - **シンプルな効果モデル**\n                    - 指数関数$1-\\exp(-\\alpha x)$\n                    - ハイパーパラメータ：$\\alpha$\n            - **学習器**\n                - **(仮)線形回帰モデル**\n        - **変換器 : 【2】**\n            - **効果のピークが、広告を打った時に限らないモデル**\n            - **ラグ(キャリーオーバー)効果モデル**\n                - **数理モデル**\n                - **効果モデル**\n                    - $x_{t}^{*}=\\frac{\\displaystyle\\sum_{l=0}^{L-1}w_{t-l}・x_{t-l}}{\\displaystyle\\sum_{l=0}^{L-1}w_{t-1}}, w_{t-l}=R^{(l-P)^{2}}$\n                    - $x_{t}$は$t$期の広告などの投入量で、$x_{t}^{*}$は$t$期とそれ以前までの広告などの効果の累積(残存・ラグ効果を加算蓄積)\n                    - **ハイパーパラメータ**\n                        - **L(length)**：効果の続く期間　※当期含む\n                        - **P(peak)**：ピークの時期（広告などを打った日の場合は0、次期は1、など）\n                        - **R(rate)**：減衰率\n            - **飽和モデル**\n                - **S字曲線を表現する関数：シグモイド関数、ロジスティック曲線、ゴンペルツ曲線**\n                - **ロジスティック曲線でのモデル**\n                    - $y=\\frac{K}{1+be^{c(x-m)}}$\n                        - **K**：上限パラメータ\n                        - **b**：形状パラメータ\n                        - **c**：形状パラメータ\n                        - **m**：位置パラメータ\n        - **予測モデル**\n            - **広告・販促は似たような時期に集中し実施**されるので、変数間で相関が発生する。**この問題の解決として正則化項を持つ線形回帰を行う**\n                - **Ridge,Lasso,ElasticNet項参照**\n        - **様々なAdStock考慮**\n            - **最適な組み合わせ**\n                - **飽和モデル** : **指数型飽和モデル・ロジスティック型飽和モデル・ゴンペルツ型飽和モデル**\n                - **ラグモデル** : **定率減少型ラグ効果モデル・ピーク可変型ラグ効果モデル**\n### **主成分回帰(PCR)**\n- 多くの広告・販促は一定の期間に集中し実施する事が多いので、相関が発生し問題が発生する。この対策の一つとして正則化付き線形回帰と**主成分分析回帰(PCR)**を使用する\n    - **主成分分析回帰(PCR)**、**元の説明変数X**に対し**主成分分析**を実施し、**主成分得点(変換後のデータ)**を求め、その**主成分得点**を使い、**線形回帰モデル**を構築する手法。\n    - 要は、**主成分分析×線形回帰**という**２つの数理モデルを組み合わせたもの** が、主成分回帰(PCR)\n- **主成分分析(PCR)について**\n    - PCAは訓練セットの分散を最大限に維持する軸を見つけ出す。\n        - 訓練セットの主成分を見つける為には、訓練セット行列$XをU W V^{T}$の3行列のドット積に分解できる\n        - **特異値分解(SVD：singular value decomposition)**という**標準的な行列分解(matrix factorization)テクニック**ここで、Vにはすべての主成分を定義する単位ベクトルを含む\n        - **PCAは、データセットが原点を中心としてセンタリングされていることを前提としている。**\n            - **scikit-learnのPCAは自動でデータのセンタリングをしてくれる**。他のライブラリを使用する場合には、データのセンタリングが自動かどうかに注意する。\n        - 全ての主成分がみつかったら、最初のd個の成分が定義する超平面に射影すれば、データセットをd次元に**次元削減**できる。\n            - これで超平面を選択すれば、射影しても分散が最大限に維持されることが保証される。\n    - **因子寄与率**\n        - **個々の主成分に沿ったデータセットの分散の分散全体に対する割合**\n        - $from sklearn.decomposition import PCA$では、$explained_variance_ratio_ $属性から、個々の主成分の**因子寄与率(explained variance ratio)**が得られる。\n            - $PCA(n_components=2)$の場合、2次元への射影になるので、因子寄与率は2であり、各値はデータセットの分散を示す。\n    - **適切な次数の選択**\n        - **次数をいくつまで削減するか**を無作為に選択するよりも、各次元に沿った因子寄与率の合計が十分な割合(例：95%)になるように次数を選択する。\n            - 可視化の為の次数の選択であれば2か3次元でよい。(スイスロール)\n            - 分散を維持する際に、オリジナルのデータセットのサイズが減る点に注意する。これは**圧縮**\n                - **圧縮(率)**によって分類アルゴリズム(SVMなど)が大幅にスピードアップされる\n            - **95%の分散を維持する**\n                - $cumsum = np.cumsum(pca.explained_variance_ratio_)$\n                - $d = np.argmax(cumsum >= 0.95) + 1$\n            - **因子寄与率のプロット**\n                - 上記の$cumsum$をプロットする\n                    - 因子寄与率の伸びが鈍化する屈曲点があるので、これがそのデータセットの本来の次数だと考えられる。\n            - **PCA射影の逆変換**\n                - 逆変換を行う事によって、次元削減されたデータセットを元の次元サイズに再構築することも可能。\n                    - 95%は5%のデータを削除しているので、元のデータが戻るわけではない。この５％は消失している。**元のデータに近いものが得られる**\n                    - オリジナルデータと再構築されたデータの平均二乗距離を**再構築誤差(reconstruction error)**\n                    - $pca = PCA(n_components=150)$\n                    - $X_reduced = pca.fit_transform(X_train)$\n                    - $X_recovered = pca.inverse_transform(X_reduced)$ ← **再構築誤差**実行\n- **主成分分析(PCR)**を挟む理由\n    - **データセットの次元を削減する**\n    - **主成分同士の相関が小さい**\n        - **主成分分析**を実施することで、**元の説明変数Xを主成分に変換**する。\n            - **主成分分析**によって生み出された**主成分**は、**お互いの相関は非常に小さいもの**となる\n        - **説明変数Xの変数の数**より少なくて済む。これが**次元削減**\n- **主成分分析(PCR)の種類**\n    - **ランダム化PCA**\n        - **scikit-learnのsvd_solver=\"randomized\"**\n        - 最初のd個の主成分の概数を高速に見つけ出してくる**ランダム化PCA**確立的アルゴリズムを使用。\n        - $O(m×n^{2})+O(n^{3})だが、ランダム化PCAはO(m×d^{2})+O(d^{3})$である為、dがnよりもかなり小さいときには大幅に高速になる\n            - $rand_pca = PCA(n_components=150, svd_solver=\"randomized\")$\n            - デフォルトは$svd_solver=\"auto\"$になっている。ｍかｎが500よりも大きく、ｄがｍかｎの80%よりも小さければ、自動的にランダム化PCAを使用する\n            - **特異値分解**を強制する場合には$svd_solver=\"full\"$っを指定する\n    - **逐次学習型PCA(IPCA)**\n        - 通常のPCAの実装には、**訓練セット全体がメモリに収まっていなければアルゴリズムを実行不可能**という問題がある。\n        - **訓練セットをミニバッチに分解し、1度に１つずつミニバッチを渡す**\n            - **大規模な訓練セットを使用する場合**や**PCAをオンライン実行(新インスタンス時の実行)**したい場合に有効\n            - $IncrementalPCA$\n            - Numpyのmemmapを使用。\n                - ディスク上のバイナリファイルに格納された大規模な配列がまるでメモリ内にあるかのように操作する。\n                - データが必要になった時に必要なだけのデータをメモリにロードする\n                - $memm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))$\n                - $batch_size = m // n_batches$\n                - $inc_pca = IncrementalPCA(n_components=150, batch_size=batch_size)$\n                - $inc_pca.fit(memm)$\n    - **カーネルPCA**\n        - **インスタンスを特徴量空間**にマッピング。**カーネルトリック**を使用。\n        - カーネルトリックを使用すれば、PCAが次元削減のために**複雑な非線形射影**を実行できる\n        - kPCAは、射影後にもインスタンスのクラスタをうまく保存でき、曲がりくねった多様体に近接するデータセットの展開にも使える\n            - **ガウスRBFカーネル**\n                - $from sklearn.decomposition import KernelPCA$\n                - $rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)$\n                - $X_reduced = rbf_pca.fit_transform(X)$\n            - **カーネルの選択とハイパーパラメータ調整**\n                - **Optuna**を使用するのが早い\n                - Otptunaを使用できない環境であれば**GridSearchやRand**を使用する。\n                - **kernel**\n                    - $\"rbf\", \"kernel\"$\n            - **再構築プレイメージ**\n                - 再構築実行\n                    - $rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04, fit_inverse_transform=True)$\n                    - $rbf_pca.fit_transform(X)$\n                    - $rbf_pca.inverse_transform(X_reduced)$\n    - **LLE(局所線形埋め込み法)**\n        - 強力な**非線形次元削減(NLDR：nonlinear dimentionality reducation)**\n        - 他のアルゴリズムとは異なり、**射影に依存しない多様体学習テクニック**\n            - 個々の訓練インスタンスが最近傍インスタンスと線形にどのような関係になるかを測定し、局所的な関係がもっとも保存される訓練セットの低次元表現を探す。\n            - **特にノイズがあまり多くない時には、曲がりくねった多様体の展開で力を発揮する**\n                - $LocallyLinearEmbedding(n_components=2, n_neighbors=10)$\n                - LLEの最初のステップは、**制約付き最適化問題**\n            - **LLEの計算量**\n                - k個の最近傍インスタンスを見つける為に、$O(m \\log(m)n \\log(k))$\n                - 重みの最適化のために$O(mnk^{3})$\n                - 低次元表現の構築のため$O(dm^{2})$、$m^{2}$で大規模なデータセットへのスケーラビリティは低い\n    - **ランダム射影**\n        - **ランダム線形射影**\n        - 次元削減の品質はインスタンス数とターゲット次元によって左右されるが、初期次元の影響を受けない\n    - **多次元尺度法(MDS)**\n        - インスタンス間の距離を維持しようとしながら次元削減を行う\n    - **Isomap**\n        - 個々のインスタンスと複数の最近棒インスタンスを結んでグラフを作り、インスタンス間の**測地距離**を可能な限り維持しながら次元を削減する\n    - **t-SNE**\n        - t分布確率的近傍埋め込み法は、類似するインスタンスを近くに保ち、似ていないインスタンスを遠くに遠ざけようとしながら次元を削減する。\n        - **可視化、特に高次空間のインスタンスのクラスタを視覚化するために使われることが多い。(MNISTの2次元可視化のような)**\n    - **LDA**\n        - これは**本来分類アルゴリズム**だが、訓練中にはクラスをもっとも特徴的に分ける軸を学習し、それらの軸を使ってデータを射影する超空間を定義する。\n        - このため、**LDAはSVM分類器などその他の分類アルゴリズムを実行する前の次元削減テクニックとして優れている**\n- **主成分回帰(PCR)のハイパーパラメータ**\n    - **主成分数**\n        - 主成分数を決める方法は様々ある。\n- **主成分分析(PCR)モデルのデメリット**\n    - モデルの説明力や予測精度は悪化する\n- **主成分分析を使用した、AdStockでの最適な組み合わせ**\n    - **ラグモデル**\n        - 定率減少型ラグ効果モデル（simple_Carryover）\n        - ピーク可変型ラグ効果モデル（peak_Carryover）\n    - **飽和モデル**\n        - 指数型飽和モデル（exp_Saturation）\n        - ロジスティック型飽和モデル（logit_Saturation）\n        - ゴンペルツ型飽和モデル（gom_Saturation）\n### 部分的最小二乗回帰(PLS)\n- **PLS回帰**とは、部分的最小2乗回帰で、主成分分析回帰PCRとの違い\n    - **PCR** : 主成分が、主成分の分散が最大になるように作成\n    - **PLS** : 主成分が、目的変数Yと主成分の共分散が最大になるように作成\n- **PLS回帰**の違い\n    - **主成分回帰(PCR)**も**PLS回帰**も、**主成分が作られる**ことは同じ\n    - **主成分回帰(PCR)**が説明変数だけで作られるのに対し、**PLS回帰**は目的変数との関係性も考慮して作られる\n- **ハイパーパラメーター：主成分の数**\n    - **PLS回帰**には、**主成分回帰（PCR）**と同様にハイパーパラメータとして**主成分の数**というものがあり、モデル構築者が与える必要がある","metadata":{}},{"cell_type":"markdown","source":"***\n## **4.モデルの調整(ハイパーパラメータ調整)**\n- **Baselineを規準にハイパーパラメータの変化の観察と再考を行う。**\n- **ハイパーパラメータ調整**\n    - **GridSearch**\n    - **RandomizedSearch**\n        - ハイパーパラメータ探査時に、n_estimatorsやepoch数のパラメータ探査を詳細に行うのは無駄な事\n            - これは**EarlyStopping**を使用して対処する。最適解が指定した数以上上がらなかった場合に停止する\n                - たいていの場合、validationセットの併用が必要\n                    - **EarlyStopping** \n    - **Optuna**\n        - **Optunaの紹介と基本原理**\n        - **Optuna**　：　サンプリングアルゴリズム\n            - **ツリー構造 Parzen アルゴリズム**\n                - optuna.samplers.TPESampler\n            - **CMA-ES アルゴリズム**\n                - optuna.samplers.CmaEsSampler\n            - **グリッド検索**\n                - optuna.samplers.GridSampler\n            - **ランダム検索**\n                - optuna.samplers.RandomSampler\n            - **Default Sampler** \n                - optuna.samplers.TPESampler\n            - ハイパーパラメータの検索範囲の定義\n                - 各ハイパーパラメータの検索範囲を含む境界変数を定義\n            - nph_warper\n                - 各パラメータは異なる方法でサンプリングされる場合がある\n            - changepoints_range\n                - 0.88888のようなパラメータは好まれないので、手動でリストを指定してOptunaサンプラーに抽出\n            - trend_reg\n                - 検索空間は、0.001から1で、ここでは対数レベル抽出法が使用される\n        - **optuna.visualization.plot_param_importances(study, evaluator=None, params=None, *, target=None, target_name='Objective Value')**\n        - ハイパーパラメータ調整結果の可視化\n            - Optunaは、チューニングプロセス中に何が起こっているのかを理解するのに役立つさまざまな可視化を提供\n            - ●どのハイパーパラメータを選択したかがより重要か、確認可能\n            - **ハイパーパラメーターの重要度をプロット**\n            >**study ( Study )** – 最適化されたスタディ。\n            >\n            >**evaluator ( Optional [ BaseImportanceEvaluator ] )** – 重要性の評価をどのアルゴリズムに基づいて行うかを指定する重要性評価器オブジェクト。デフォルトはFanovaImportanceEvaluatorです。。\n            >\n            >**params (Optional[List[str]])** – 評価するパラメータ名のリスト。None の場合、完了したすべてのトライアルで存在するすべてのパラメータが評価される。\n            >\n            >**target (Optional[Callable[[FrozenTrial], float]])** – 表示する値を指定する関数です。None で、study が単一目的最適化に使用されている場合、目的値がプロットされる。\n            >\n            >                          study が多目的最適化に使用される場合、この引数を指定する。\n            >                          例えば、第一目的のハイパーパラメータの重要度を得るには、 target=lambda t: t.values[0] をターゲットパラメータに使用します。\n            >**target_name (str)** - 軸ラベルに表示するターゲットの名前。    \n        - **optuna.visualization.plot_optimization_history(study,  *,  target=None, target_name='Objective Value', error_bar=False)**\n            - 調整パラメータの履歴を表示する。Plotlyで表示する事によって、どのObject Valueがベストであったかが可視化できる。\n            - **スタディ内のすべての試行の最適化履歴をプロット**\n            > **(Union[Study,Sequence[Study]])** - 目標値に対して試行がプロットされたStudyオブジェクト。これらの最適化履歴を比較したい場合は、複数のStudyを渡すことができます。\n            >\n            > **target (Optional[Callable[[FrozenTrial],float]])** - 表示する値を指定する関数。Noneでstudyが単一目的最適化に使用されている場合、目的値がプロットされる。\n            >\n            >                                        study が多目的最適化に使用される場合、この引数を指定する。\n            > **target_name (str)** – 軸ラベルと凡例に表示されるターゲットの名前です\n            >\n            > **error_bar (bool)** - エラーバーを表示するためのフラグです。\n        - **optuna.visualization.plot_parallel_coordinate(study, params=None, *, target=None, target_name='Objective Value')**\n            - 研究の高次元のパラメータ関係をプロットする。\n            - パラメータに欠損値がある場合、欠損値のある試験はプロットされないことに注意。\n            > **study (Study)** – 試行が目標値に対してプロットされるStudyオブジェクト。\n            >\n            > **params (Optional[List[str]])** – 可視化するパラメータリスト。デフォルトは全パラメーターです。\n            >\n            > **target (Optional[Callable[[FrozenTrial], float]])** – 表示する値を指定する関数です。None で、study が単一目的最適化に使用されている場合、目的値がプロットされる。\n            >                                study が多目的最適化に使用される場合、この引数を指定する。\n            >\n            > **target_name (str)** – 軸ラベルと凡例に表示するターゲットの名前。\n- **k分割交差検証**\n    - **cross_val_score()**\n        - [ライブラリ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)\n    - **KFold()**\n        - [ライブラリ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n        - トレーニング/テスト セットのデータを分割するためのトレーニング/テスト インデックスを提供します。データセットを k 個の連続したフォールドに分割します (デフォルトではシャッフルなし)\n            - KFoldすべてのサンプルを分割します。$k$フォールドと呼ばれるサンプルのグループ$(k=n　これはLeave One Out戦略と同等)$\n            - 等しいサイズ (可能な場合)。予測関数は、$k-1$折りたたまれ、残った折り目がテストに使用されます。\n    - **RepeatedKFold()**\n        - [ライブラリ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html)\n        - n 回繰り返し、n回実行する必要がある場合に使用できKFold、繰り返しごとに異なる分割を生成します。\n            - 2回繰り返される2倍のK-Fold\n    - **cross_validate**\n        - [ライブラリ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate)\n        - 複数のメトリクスで相互検証を実行し、トレーニング スコア、適合時間、およびスコア時間を返す\n        - 評価のために複数のメトリックを指定できます。\n        - テスト スコアに加えて、適合時間、スコア時間 (およびオプションでトレーニング スコアと適合推定量) を含む dict を返します。\n            - スコアリング パラメータが文字列、呼び出し可能、または None である単一メトリック評価の場合、キーは次のようになります。<'test_score', 'fit_time', 'score_time'>\n- **EarlyStopping**","metadata":{}},{"cell_type":"markdown","source":"***\n## **4.1.Logging**\n- **ログ調整**\n    - **Prophet Logging**","metadata":{}},{"cell_type":"markdown","source":"***\n## **4.2. 性能指標**\n- **予測精度の評価指標**\n    - $y_{i}^{actual}・・・i番目の実測値$\n    - $y_{i}^{pred}・・・i番目の予測値$\n    - $n・・・実測値,予測値の数$\n        - **RMSE/MAE/MAPE**\n            - **二乗平均平方根誤差(RMSE、Root Mean Squared Error)**\n                - $\\displaystyle\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}^{actual}-y_{i}^{pred})^{2}}$\n            - **平均絶対誤差(MAE、Mean Absolute Error)**\n                - $\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}^{actual}-y_{i}^{pred}|$\n            - **平均絶対パーセント誤差（MAPE、Mean absolute percentage error**\n                - $\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}|\\frac{y_{i}^{actual}-y_{i}^{actual}}{y_{i}^{actual}}|$","metadata":{}},{"cell_type":"markdown","source":"***\n## **5. 意思決定、制御**\n- 長期的なデータ予測を適用した制御　(制御)\n    - 目的に応じた数理最適化 (在庫適正化、電力供給最適化)　(意思決定、最適化)   \n        - デプロイのタイミングで確率分布が途中で変化する事が予想される、自己回帰モデルは、主に自然科学的なデータセットでしか効果を発揮しない場合がある。\n            - このような問題が発生する時には、パターンマッチング的な考え方(LSTM、DLモデル)や様々なモデルを組み合わせられる状態空間モデルなどを利用する事が多い。","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}